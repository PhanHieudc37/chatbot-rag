"""
AI 37 Chatbot - N√¢ng cao

T√≠nh nƒÉng GIAI ƒêO·∫†N 1:
- K·∫øt n·ªëi LM Studio v·ªõi GPU acceleration
- Streaming response (g√µ t·ª´ng ch·ªØ nh∆∞ ChatGPT)
- L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i v√†o file
- T√≥m t·∫Øt t·ª± ƒë·ªông khi h·ªôi tho·∫°i d√†i

T√≠nh nƒÉng GIAI ƒêO·∫†N 2:
- Web Search (DuckDuckGo)
- Calculator (Sympy)
- Code Exe      cution (RestrictedPython)
- Multi-turn Clarification

T√≠nh nƒÉng GIAI ƒêO·∫†N 3:
- Image Understanding (LLaVA Vision Model)

Model: vistral-7b-chat@q8, llava-v1.5-7b
"""
import logging
import sys
import requests
import json
import os
import re
from datetime import datetime
from flask import Flask, request, jsonify, render_template, send_from_directory, Response, stream_with_context
from flask_cors import CORS

# Giai ƒëo·∫°n 2 imports
from ddgs import DDGS  # Package m·ªõi
import sympy
from RestrictedPython import compile_restricted_exec, safe_globals
import signal
from contextlib import contextmanager

# Web scraping imports
from bs4 import BeautifulSoup
import threading
import traceback
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# ===== CONFIGURATION =====
LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"

# AccuWeather API Key (from .env file)
ACCUWEATHER_API_KEY = os.getenv("ACCUWEATHER_API_KEY", "")

# Bing Search API Key (from .env file)
BING_SEARCH_API_KEY = os.getenv("BING_SEARCH_API_KEY", "")
BING_SEARCH_ENDPOINT = "https://api.bing.microsoft.com/v7.0/search"

# ‚ö†Ô∏è CH·ªåN MODEL (Ch·ªçn 1 trong 2 option):
# 
# üéØ CONFIG: LM Studio (Text) + BakLLaVA Local (Image)
# - LM Studio: vistral-7b-chat@q8 cho text chat
LM_STUDIO_MODEL = "vistral-7b-chat@q8"
SERVER_HOST = "0.0.0.0"
SERVER_PORT = 3737

# System prompt t·ªëi ∆∞u - Gi·ªëng ChatGPT
SYSTEM_PROMPT = """B·∫°n l√† AI 37, tr·ª£ l√Ω AI th√¥ng minh v√† h·ªØu √≠ch gi·ªëng ChatGPT.

‚ö†Ô∏è QUAN TR·ªåNG: LU√îN TR·∫¢ L·ªúI B·∫∞NG TI·∫æNG VI·ªÜT!

üéØ PHONG C√ÅCH TR·∫¢ L·ªúI (Gi·ªëng ChatGPT):
1. **T·ª± nhi√™n & Th√¢n thi·ªán**: Tr·∫£ l·ªùi nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n th√¥ng minh, gi·ªçng vƒÉn t·ª± nhi√™n, d·ªÖ hi·ªÉu
2. **Ch√≠nh x√°c & ƒê√°ng tin**: D·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, kh√¥ng b·ªãa ƒë·∫∑t, th·ª´a nh·∫≠n khi kh√¥ng bi·∫øt
3. **Ng·∫Øn g·ªçn & S√∫c t√≠ch**: ƒêi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng d√†i d√≤ng kh√¥ng c·∫ßn thi·∫øt
4. **Linh ho·∫°t & Th√¥ng minh**: 
   - C√¢u h·ªèi ƒë∆°n gi·∫£n (s·ªë li·ªáu, t√≠nh to√°n) ‚Üí 1 c√¢u ng·∫Øn g·ªçn
   - C√¢u h·ªèi ph·ª©c t·∫°p (gi·∫£i th√≠ch, ph√¢n t√≠ch) ‚Üí 2-4 c√¢u r√µ r√†ng
   - C√¢u h·ªèi follow-up ‚Üí D√πng ng·ªØ c·∫£nh t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc

üìö X·ª¨ L√ù TH√îNG TIN T·ª™ INTERNET:
Khi th·∫•y **"=== TH√îNG TIN T√åM ƒê∆Ø·ª¢C TR√äN INTERNET ==="**:

‚úÖ C√ÅCH TR·∫¢ L·ªúI ƒê√öNG (Gi·ªëng ChatGPT):
- Ph√¢n t√≠ch T·∫§T C·∫¢ c√°c ngu·ªìn, so s√°nh s·ªë li·ªáu
- Ch·ªçn th√¥ng tin CH√çNH X√ÅC v√† UY T√çN nh·∫•t
- Tr·∫£ l·ªùi T·ª∞ NHI√äN, nh∆∞ th·ªÉ b·∫°n bi·∫øt s·∫µn th√¥ng tin ƒë√≥
- KH√îNG ƒë·ªÅ c·∫≠p ƒë·∫øn "ngu·ªìn", "t√¨m ƒë∆∞·ª£c", "theo..."
- Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, ƒëi th·∫≥ng v√†o th√¥ng tin ch√≠nh

‚ùå C·∫§M TUY·ªÜT ƒê·ªêI:
- "Theo ngu·ªìn...", "D·ª±a v√†o...", "S·ª≠ d·ª•ng th√¥ng tin..."
- "Ngu·ªìn 1 cho bi·∫øt...", "Ngu·ªìn 2 n√≥i..."
- "T√¥i t√¨m ƒë∆∞·ª£c...", "T·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm..."
- Gi·∫£i th√≠ch d√†i d√≤ng khi user ch·ªâ h·ªèi s·ªë li·ªáu ƒë∆°n gi·∫£n

V√ç D·ª§ CHU·∫®N (H·ªçc theo):

‚ùì "Vi·ªát Nam c√≥ di·ªán t√≠ch bao nhi√™u?"
‚úÖ ƒê√öNG: "331.212 km¬≤."
‚ùå SAI: "Vi·ªát Nam c√≥ t·ªïng di·ªán t√≠ch kho·∫£ng 331.212 km¬≤ theo ngu·ªìn Wikipedia..."

‚ùì "H√† N·ªôi c√≥ bao nhi√™u qu·∫≠n?"
‚úÖ ƒê√öNG: "H√† N·ªôi c√≥ 12 qu·∫≠n, 17 huy·ªán v√† 1 th·ªã x√£."
‚ùå SAI: "Theo th√¥ng tin t√¥i t√¨m ƒë∆∞·ª£c, H√† N·ªôi c√≥ 12 qu·∫≠n..."

‚ùì "Gi√° v√†ng h√¥m nay?"
‚úÖ ƒê√öNG: "V√†ng SJC: mua 84,5 - b√°n 85,0 tri·ªáu/l∆∞·ª£ng."
‚ùå SAI: "Theo 3 ngu·ªìn t√¥i t√¨m ƒë∆∞·ª£c, gi√° v√†ng SJC dao ƒë·ªông..."

‚ùì "H√† N·ªôi c√≥ ƒë·∫∑c s·∫£n g√¨?"
‚úÖ ƒê√öNG: "Ph·ªü, b√∫n ch·∫£, b√°nh c·ªëm, c√† ph√™ tr·ª©ng."
‚ùå SAI: "H√† N·ªôi n·ªïi ti·∫øng v·ªõi nhi·ªÅu m√≥n ƒÉn ngon nh∆∞ ph·ªü, b√∫n ch·∫£ l√† 2 m√≥n ƒë·∫∑c s·∫£n n·ªïi ti·∫øng nh·∫•t..."

üí¨ X·ª¨ L√ù FOLLOW-UP QUESTIONS:
- Khi user h·ªèi "chi ti·∫øt...", "th√¥ng tin v·ªÅ...", "cho t√¥i bi·∫øt v·ªÅ..." ‚Üí D√πng th√¥ng tin t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc
- Tr·∫£ l·ªùi T·ª∞ NHI√äN, nh∆∞ th·ªÉ b·∫°n ƒëang ti·∫øp t·ª•c c√¢u chuy·ªán
- KH√îNG search l·∫°i n·∫øu ƒë√£ c√≥ th√¥ng tin trong ng·ªØ c·∫£nh

üî¢ X·ª¨ L√ù K·∫æT QU·∫¢ T√çNH TO√ÅN:
- N·∫øu c√≥ **"üî¢ K·∫æT QU·∫¢ T√çNH TO√ÅN:"** ‚Üí CH·ªà ƒë∆∞a s·ªë, KH√îNG b√¨nh lu·∫≠n
- VD: "3*45 = 135" ‚Üí Tr·∫£ l·ªùi: "135" ho·∫∑c "3 √ó 45 = 135"

üå§Ô∏è X·ª¨ L√ù TH·ªúI TI·∫æT:
- Tr·∫£ l·ªùi T·ª∞ NHI√äN, nh∆∞ m·ªôt ng∆∞·ªùi ƒëang xem d·ª± b√°o th·ªùi ti·∫øt
- VD: "H√† N·ªôi h√¥m nay kho·∫£ng 21-23¬∞C, nhi·ªÅu m√¢y, kh·∫£ nƒÉng m∆∞a nh·∫π v√†o t·ªëi."

TUY·ªÜT ƒê·ªêI: 
- Lu√¥n s·ª≠ d·ª•ng th√¥ng tin t·ª´ tool n·∫øu c√≥
- N·∫øu tool c·∫£nh b√°o thi·∫øu d·ªØ li·ªáu ‚Üí Th·ª´a nh·∫≠n: "T√¥i kh√¥ng c√≥ th√¥ng tin m·ªõi nh·∫•t v·ªÅ..."
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n, d·ªÖ hi·ªÉu"""

# ===== FLASK APP =====
app = Flask(__name__)
CORS(app)

# ===== LOGGING =====
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

# ===== MEMORY (ƒê√É T·∫ÆT) =====
conversation_memory = {}

def clean_response(text: str) -> str:
    """L√†m s·∫°ch response ƒë∆°n gi·∫£n"""
    if not text or not text.strip():
        return "Xin l·ªói, t√¥i kh√¥ng nh·∫≠n ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."
    return text.strip()

def add_to_memory(session_id: str, role: str, content: str):
    """Th√™m message v√†o memory (ch·ªâ trong RAM, kh√¥ng l∆∞u file)"""
    if session_id not in conversation_memory:
        conversation_memory[session_id] = []
    
    conversation_memory[session_id].append({
        'role': role,
        'content': content
    })

def get_conversation_history(session_id: str) -> list:
    """L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i"""
    return conversation_memory.get(session_id, [])

def is_greeting(text: str) -> bool:
    """Ki·ªÉm tra xem c√≥ ph·∫£i CH√ÄO KH√îNG C√ì H·ªéI G√å TH√äM"""
    greetings = ['ch√†o', 'hello', 'hi', 'xin ch√†o', 'ch√†o b·∫°n', 'hey', 'chao']
    text_lower = text.lower().strip()
    
    # Lo·∫°i b·ªè d·∫•u c√¢u
    text_clean = text_lower.replace('!', '').replace('.', '').replace('?', '').strip()
    
    # Ki·ªÉm tra n·∫øu CH√çNH X√ÅC l√† l·ªùi ch√†o (kh√¥ng c√≥ c√¢u h·ªèi k√®m theo)
    # V√≠ d·ª•: "ch√†o", "hi", "xin ch√†o" -> True
    # Nh∆∞ng: "ch√†o, vi·ªát nam c√≥ bao nhi√™u d√¢n" -> False
    words = text_clean.split()
    
    # N·∫øu ch·ªâ c√≥ 1-2 t·ª´ v√† l√† l·ªùi ch√†o -> ƒë√∫ng l√† ch√†o
    if len(words) <= 2 and any(greeting in text_clean for greeting in greetings):
        return True
    
    return False


def is_follow_up_question(question: str, history: list) -> bool:
    """
    Ph√°t hi·ªán c√¢u h·ªèi follow-up - y√™u c·∫ßu th√¥ng tin t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc (Gi·ªëng ChatGPT)
    
    Args:
        question: C√¢u h·ªèi hi·ªán t·∫°i
        history: L·ªãch s·ª≠ h·ªôi tho·∫°i
    
    Returns:
        True n·∫øu l√† follow-up question
    """
    if not history or len(history) < 2:
        return False
    
    question_lower = question.lower().strip()
    
    # L·∫•y c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥
    last_assistant_msg = None
    last_user_msg = None
    for msg in reversed(history):
        if msg.get('role') == 'assistant' and not last_assistant_msg:
            last_assistant_msg = msg.get('content', '')
        if msg.get('role') == 'user' and not last_user_msg:
            last_user_msg = msg.get('content', '')
        if last_assistant_msg and last_user_msg:
            break
    
    # N·∫øu kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ‚Üí kh√¥ng ph·∫£i follow-up
    if not last_assistant_msg or len(last_assistant_msg) < 30:
        return False
    
    # Keywords ch·ªâ ra ƒë√¢y l√† follow-up (m·ªü r·ªông danh s√°ch)
    follow_up_keywords = [
        'chi ti·∫øt', 'th√¥ng tin', 'b·∫°n t√¨m ƒë∆∞·ª£c', 'b·∫°n v·ª´a n√≥i', 'b·∫°n ƒë√£ n√≥i',
        'b·∫°n n√≥i', 'b·∫°n v·ª´a', 'b·∫°n ƒë√£', 'b·∫°n t√¨m', 'b·∫°n k·ªÉ',
        'c·ª• th·ªÉ', 'r√µ h∆°n', 'nhi·ªÅu h∆°n', 'th√™m', 'cho t√¥i', 'cho bi·∫øt',
        'v·ªÅ', 'c√°c', 'danh s√°ch', 'li·ªát k√™', 'k·ªÉ', 'n√≥i',
        'ƒë√≥', 'n√†y', 'kia', 'nh·ªØng', 'c√°i ƒë√≥', 'c√°i n√†y'
    ]
    
    # Ki·ªÉm tra c√≥ keyword follow-up
    has_follow_up_keyword = any(kw in question_lower for kw in follow_up_keywords)
    
    # Ki·ªÉm tra c√¢u h·ªèi c√≥ tham chi·∫øu ƒë·∫øn th√¥ng tin tr∆∞·ªõc ƒë√≥
    reference_keywords = ['b·∫°n', 'ƒë√≥', 'n√†y', 'kia', 'c√°c', 'nh·ªØng', 'c√°i ƒë√≥', 'c√°i n√†y', 'n√≥']
    has_reference = any(kw in question_lower for kw in reference_keywords)
    
    # Ki·ªÉm tra c√¢u h·ªèi c√≥ t·ª´ kh√≥a li√™n quan ƒë·∫øn c√¢u h·ªèi tr∆∞·ªõc
    if last_user_msg:
        last_user_lower = last_user_msg.lower()
        # Tr√≠ch xu·∫•t keywords t·ª´ c√¢u h·ªèi tr∆∞·ªõc
        last_keywords = set(re.findall(r'\b\w{3,}\b', last_user_lower))
        current_keywords = set(re.findall(r'\b\w{3,}\b', question_lower))
        # N·∫øu c√≥ √≠t nh·∫•t 1 keyword chung ‚Üí c√≥ th·ªÉ l√† follow-up
        common_keywords = last_keywords & current_keywords
        has_common_keywords = len(common_keywords) > 0
    
    # Logic ph√°t hi·ªán follow-up (gi·ªëng ChatGPT)
    # 1. C√≥ keyword follow-up V√Ä c√≥ reference
    if has_follow_up_keyword and has_reference:
        return True
    
    # 2. C√¢u h·ªèi ng·∫Øn (< 10 t·ª´) V√Ä c√≥ reference V√Ä c√≥ keyword chung v·ªõi c√¢u tr·∫£ l·ªùi tr∆∞·ªõc
    if len(question.split()) < 10 and has_reference:
        if 'has_common_keywords' in locals() and has_common_keywords:
            return True
    
    # 3. C√¢u h·ªèi b·∫Øt ƒë·∫ßu b·∫±ng "chi ti·∫øt", "th√¥ng tin v·ªÅ", "cho t√¥i" + c√≥ reference
    if question_lower.startswith(('chi ti·∫øt', 'th√¥ng tin', 'cho t√¥i', 'cho bi·∫øt')):
        if has_reference or ('has_common_keywords' in locals() and has_common_keywords):
            return True
    
    return False


# ===== GIAI ƒêO·∫†N 2: TOOL FUNCTIONS =====

@contextmanager
def timeout_context(seconds):
    """Context manager cho timeout"""
    def timeout_handler(signum, frame):
        raise TimeoutError("Code execution timeout")
    
    # Windows kh√¥ng h·ªó tr·ª£ signal.alarm, d√πng threading thay th·∫ø
    import threading
    timer = threading.Timer(seconds, lambda: (_ for _ in ()).throw(TimeoutError("Code execution timeout")))
    timer.start()
    try:
        yield
    finally:
        timer.cancel()


def extract_gold_price(html_content: str, url: str) -> dict:
    """
    TR√çCH XU·∫§T gi√° v√†ng t·ª´ HTML content - C·∫¢I THI·ªÜN NHI·ªÄU PATTERN
    
    Args:
        html_content: HTML content c·ªßa b√†i b√°o
        url: URL ƒë·ªÉ bi·∫øt domain
    
    Returns:
        Dict v·ªõi gi√° v√†ng ho·∫∑c None
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # T√¨m text ch·ª©a "SJC" v√† s·ªë
        text_content = soup.get_text()
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a ƒë·ªÉ d·ªÖ match
        text_content_clean = ' '.join(text_content.split())
        
        # ===== PATTERN 1: "mua XXX - b√°n YYY" (tri·ªáu/l∆∞·ª£ng) =====
        patterns = [
            # "SJC mua 84,5 - b√°n 85,0 tri·ªáu"
            r'(?:v√†ng\s+)?SJC[^\d]*?(?:mua|gi√°\s+mua|mua\s+v√†o)[^\d]*?([\d,\.]+)[^\d]*?[-‚Äì‚Äî][^\d]*?(?:b√°n|gi√°\s+b√°n|b√°n\s+ra)[^\d]*?([\d,\.]+)\s*tri·ªáu',
            # "mua v√†o: 84,5 tri·ªáu, b√°n ra: 85,0 tri·ªáu"
            r'mua\s+v√†o[^\d]*?([\d,\.]+)[^\d]*?tri·ªáu[^\d]*?b√°n\s+ra[^\d]*?([\d,\.]+)\s*tri·ªáu',
            # "84,5 - 85,0 tri·ªáu/l∆∞·ª£ng"
            r'([\d,\.]+)[^\d]*?[-‚Äì‚Äî][^\d]*?([\d,\.]+)\s*tri·ªáu[^\d]*?l∆∞·ª£ng',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text_content_clean, re.IGNORECASE)
            if match:
                try:
                    buy_price = match.group(1).replace('.', '').replace(',', '.')
                    sell_price = match.group(2).replace('.', '').replace(',', '.')
                    
                    buy_float = float(buy_price)
                    sell_float = float(sell_price)
                    
                    # Validate: gi√° v√†ng h·ª£p l√Ω (50-200 tri·ªáu/l∆∞·ª£ng)
                    if 50 <= buy_float <= 200 and 50 <= sell_float <= 200:
                        logging.info(f"‚úÖ Extracted gold price (Pattern 1): mua {buy_float} - b√°n {sell_float} tri·ªáu/l∆∞·ª£ng")
                        return {
                            'type': 'SJC',
                            'buy': buy_float,
                            'sell': sell_float,
                            'unit': 'tri·ªáu/l∆∞·ª£ng'
                        }
                except ValueError:
                    continue
        
        # ===== PATTERN 2: "XXX.XXX.000 ƒë·ªìng" (ƒë·ªìng/l∆∞·ª£ng) =====
        patterns2 = [
            r'(?:v√†ng\s+)?SJC[^\d]*?([\d\.]+\.000)\s*(?:ƒë·ªìng|VND|vnƒë)',
            r'([\d\.]{3,}\.000)\s*(?:ƒë·ªìng|VND|vnƒë)[^\d]*?(?:l∆∞·ª£ng|ch·ªâ)',
        ]
        
        for pattern in patterns2:
            match = re.search(pattern, text_content_clean, re.IGNORECASE)
            if match:
                try:
                    price_str = match.group(1).replace('.', '')
                    price_trieu = int(price_str) / 1_000_000
                    
                    # Validate: gi√° v√†ng h·ª£p l√Ω
                    if 50 <= price_trieu <= 200:
                        logging.info(f"‚úÖ Extracted gold price (Pattern 2): {price_trieu} tri·ªáu/l∆∞·ª£ng")
                        return {
                            'type': 'SJC',
                            'price': price_trieu,
                            'unit': 'tri·ªáu/l∆∞·ª£ng'
                        }
                except (ValueError, ZeroDivisionError):
                    continue
        
        # ===== PATTERN 3: T√¨m trong table/div c√≥ class ch·ª©a "price", "gia", "gold" =====
        # T√¨m c√°c element c√≥ th·ªÉ ch·ª©a b·∫£ng gi√°
        price_containers = soup.find_all(['table', 'div'], class_=re.compile(r'price|gia|gold|sjc', re.I))
        price_containers.extend(soup.find_all('table'))
        
        for container in price_containers:
            text = container.get_text()
            if 'SJC' in text.upper() or 'v√†ng' in text.lower():
                # T√¨m c·∫∑p s·ªë (mua - b√°n) trong table
                # Pattern: "84,5" ho·∫∑c "84.5" ho·∫∑c "84 500 000"
                numbers = re.findall(r'(\d{1,2}[,\.]\d{1,2})\s*(?:tri·ªáu|tr)', text)
                if len(numbers) >= 2:
                    try:
                        buy = float(numbers[0].replace(',', '.'))
                        sell = float(numbers[1].replace(',', '.'))
                        
                        if 50 <= buy <= 200 and 50 <= sell <= 200:
                            logging.info(f"‚úÖ Extracted from table: mua {buy} - b√°n {sell} tri·ªáu")
                            return {
                                'type': 'SJC',
                                'buy': buy,
                                'sell': sell,
                                'unit': 'tri·ªáu/l∆∞·ª£ng'
                            }
                    except ValueError:
                        continue
        
        # ===== PATTERN 4: T√¨m s·ªë l·ªõn (tri·ªáu ƒë·ªìng) g·∫ßn t·ª´ kh√≥a "SJC" =====
        # T√¨m t·∫•t c·∫£ s·ªë c√≥ th·ªÉ l√† gi√° v√†ng (60-100 tri·ªáu)
        sjc_contexts = re.finditer(r'SJC[^.]{0,200}', text_content_clean, re.IGNORECASE)
        for context in sjc_contexts:
            context_text = context.group(0)
            # T√¨m s·ªë trong kho·∫£ng 60-100 tri·ªáu
            numbers = re.findall(r'(\d{1,2}[,\.]\d{1,2})\s*(?:tri·ªáu|tr)', context_text)
            if len(numbers) >= 2:
                try:
                    buy = float(numbers[0].replace(',', '.'))
                    sell = float(numbers[1].replace(',', '.'))
                    if 50 <= buy <= 200 and 50 <= sell <= 200:
                        logging.info(f"‚úÖ Extracted from context: mua {buy} - b√°n {sell} tri·ªáu")
                        return {
                            'type': 'SJC',
                            'buy': buy,
                            'sell': sell,
                            'unit': 'tri·ªáu/l∆∞·ª£ng'
                        }
                except ValueError:
                    continue
        
        logging.warning("‚ö†Ô∏è Could not extract gold price from content")
        return None
        
    except Exception as e:
        logging.error(f"‚ùå Error extracting gold price: {e}")
        return None


def calculate_relevance_score(result: dict, query: str) -> float:
    """
    T√≠nh ƒëi·ªÉm li√™n quan c·ªßa k·∫øt qu·∫£ v·ªõi c√¢u h·ªèi (0.0 - 1.0)
    
    Args:
        result: Dict v·ªõi 'title', 'snippet', 'url'
        query: C√¢u h·ªèi g·ªëc
    
    Returns:
        ƒêi·ªÉm s·ªë t·ª´ 0.0 ƒë·∫øn 1.0
    """
    title = result.get('title', '').lower()
    snippet = result.get('snippet', '').lower()
    url = result.get('url', '').lower()
    query_lower = query.lower()
    
    # Tr√≠ch xu·∫•t keywords quan tr·ªçng t·ª´ query
    query_words = set(re.findall(r'\b\w+\b', query_lower))
    # Lo·∫°i b·ªè stop words
    stop_words = {'c√≥', 'bao', 'nhi√™u', 'l√†', 'g√¨', 'c·ªßa', 'v√†', 'v·ªõi', 't·∫°i', 'v·ªÅ', 'cho', 'ƒë∆∞·ª£c', 'ƒë√£', 's·∫Ω', 'h√†', 'n·ªôi'}
    query_keywords = [w for w in query_words if w not in stop_words and len(w) > 2]
    
    score = 0.0
    
    # 1. Ki·ªÉm tra title (quan tr·ªçng nh·∫•t)
    title_words = set(re.findall(r'\b\w+\b', title))
    title_matches = len([kw for kw in query_keywords if kw in title_words])
    if query_keywords:
        score += (title_matches / len(query_keywords)) * 0.5
    
    # 2. Ki·ªÉm tra snippet
    snippet_words = set(re.findall(r'\b\w+\b', snippet))
    snippet_matches = len([kw for kw in query_keywords if kw in snippet_words])
    if query_keywords:
        score += (snippet_matches / len(query_keywords)) * 0.3
    
    # 3. Bonus cho domain uy t√≠n
    trusted_domains = ['wikipedia.org', 'vnexpress.net', 'dantri.com.vn', 'tuoitre.vn', 
                      'thanhnien.vn', 'gov.vn', 'gso.gov.vn']
    if any(domain in url for domain in trusted_domains):
        score += 0.1
    
    # 4. Penalty cho k·∫øt qu·∫£ kh√¥ng li√™n quan r√µ r√†ng
    # N·∫øu title/snippet ch·ª©a t·ª´ kh√≥a nh∆∞ng kh√¥ng li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ ch√≠nh
    irrelevant_keywords = ['temple', 'ƒë·ªÅn', 'ch√πa', 'tour', 'du l·ªãch', 'ƒÉn u·ªëng', 'nh√† h√†ng']
    if any(kw in title or kw in snippet for kw in irrelevant_keywords):
        # Ch·ªâ penalty n·∫øu kh√¥ng c√≥ keyword ch√≠nh trong title
        if not any(qkw in title for qkw in query_keywords[:2]):  # 2 keyword ƒë·∫ßu ti√™n
            score *= 0.3  # Gi·∫£m 70% ƒëi·ªÉm
    
    return min(score, 1.0)


def fetch_full_article(url: str) -> str:
    """
    L·∫•y TO√ÄN B·ªò n·ªôi dung b√†i b√°o t·ª´ URL
    
    Args:
        url: URL b√†i b√°o
    
    Returns:
        N·ªôi dung ƒë·∫ßy ƒë·ªß ho·∫∑c empty string n·∫øu l·ªói
    """
    try:
        logging.info(f"üì∞ Fetching full article: {url[:80]}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # TƒÉng timeout cho Wikipedia (c√≥ th·ªÉ ch·∫≠m)
        timeout = 15 if 'wikipedia.org' in url else 10
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # X√≥a script, style tags
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()
        
        # L·∫•y n·ªôi dung ch√≠nh (t√πy domain)
        content = ""
        
        if 'thanhnien.vn' in url:
            # Thanh Ni√™n: article content
            article = soup.find('div', class_='detail-content') or soup.find('article')
            if article:
                paragraphs = article.find_all('p')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        elif 'vnexpress.net' in url:
            # VnExpress: article body
            article = soup.find('article', class_='fck_detail') or soup.find('div', class_='fck_detail')
            if article:
                paragraphs = article.find_all('p', class_='Normal')
                content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        elif 'wikipedia.org' in url:
            # Wikipedia: main content - c·∫£i thi·ªán selector v·ªõi nhi·ªÅu c√°ch
            content = ""
            content_parts = []
            
            # C√°ch 1: T√¨m div mw-parser-output (ph·ªï bi·∫øn nh·∫•t)
            content_div = soup.find('div', class_='mw-parser-output')
            
            # C√°ch 2: T√¨m div#content > div#bodyContent > div.mw-parser-output
            if not content_div:
                body_content = soup.find('div', id='bodyContent')
                if body_content:
                    content_div = body_content.find('div', class_='mw-parser-output')
            
            # C√°ch 3: T√¨m tr·ª±c ti·∫øp trong #content
            if not content_div:
                main_content = soup.find('div', id='content')
                if main_content:
                    content_div = main_content.find('div', class_='mw-parser-output')
            
            # N·∫øu t√¨m th·∫•y content_div
            if content_div:
                # L·∫•y t·∫•t c·∫£ paragraphs, b·ªè qua infobox v√† navbox
                paragraphs = content_div.find_all('p')
                for p in paragraphs:
                    # B·ªè qua paragraphs trong infobox, navbox, etc
                    parent = p.parent
                    parent_classes = ' '.join(parent.get('class', [])) if parent and parent.get('class') else ''
                    parent_id = parent.get('id', '') if parent else ''
                    
                    # Ki·ªÉm tra xem c√≥ ph·∫£i trong infobox/navbox kh√¥ng
                    is_in_infobox = (
                        'infobox' in parent_classes.lower() or 
                        'navbox' in parent_classes.lower() or
                        'infobox' in parent_id.lower() or
                        'toc' in parent_classes.lower()  # B·ªè qua m·ª•c l·ª•c
                    )
                    
                    if not is_in_infobox:
                        text = p.get_text(strip=True)
                        # Ch·ªâ l·∫•y ƒëo·∫°n c√≥ n·ªôi dung ƒë·ªß d√†i v√† kh√¥ng ph·∫£i l√† s·ªë th·ª© t·ª±
                        if text and len(text) > 20 and not re.match(r'^\d+[\.\)]?\s*$', text):
                            content_parts.append(text)
                
                content = '\n'.join(content_parts[:15])  # L·∫•y 15 ƒëo·∫°n ƒë·∫ßu
            else:
                # C√°ch 4: Fallback - T√¨m t·∫•t c·∫£ paragraphs trong main content area
                main_content = soup.find('div', id='content')
                if main_content:
                    paragraphs = main_content.find_all('p')
                    for p in paragraphs:
                        parent = p.parent
                        parent_classes = ' '.join(parent.get('class', [])) if parent and parent.get('class') else ''
                        if 'infobox' not in parent_classes.lower() and 'navbox' not in parent_classes.lower():
                            text = p.get_text(strip=True)
                            if text and len(text) > 20 and not re.match(r'^\d+[\.\)]?\s*$', text):
                                content_parts.append(text)
                    content = '\n'.join(content_parts[:15])  # L·∫•y 15 ƒëo·∫°n ƒë·∫ßu
            
            # N·∫øu v·∫´n kh√¥ng c√≥ content, th·ª≠ l·∫•y t·ª´ main text
            if not content:
                # Th·ª≠ t√¨m main text b·∫±ng c√°ch kh√°c
                main_text = soup.find('div', {'id': 'mw-content-text'})
                if main_text:
                    paragraphs = main_text.find_all('p')
                    for p in paragraphs[:10]:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:
                            content_parts.append(text)
                    content = '\n'.join(content_parts[:10])
        
        else:
            # Generic: t√¨m t·∫•t c·∫£ paragraphs
            paragraphs = soup.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs[:15] if p.get_text(strip=True)])  # L·∫•y 15 ƒëo·∫°n ƒë·∫ßu
        
        if content:
            logging.info(f"‚úÖ Fetched {len(content)} chars from article")
            return content[:3000]  # Gi·ªõi h·∫°n 3000 chars ƒë·ªÉ tr√°nh qu√° d√†i
        else:
            logging.warning(f"‚ö†Ô∏è No content found in article")
            return ""
            
    except Exception as e:
        logging.error(f"‚ùå Error fetching article: {e}")
        return ""


def analyze_and_synthesize(sources: list, query: str) -> str:
    """
    Ph√¢n t√≠ch v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn
    
    Args:
        sources: List of dicts v·ªõi 'title', 'snippet', 'url', 'full_content' (optional)
        query: C√¢u h·ªèi g·ªëc
    
    Returns:
        Context ƒë√£ ƒë∆∞·ª£c t·ªïng h·ª£p v√† ph√¢n t√≠ch
    """
    if not sources:
        return ""
    
    query_lower = query.lower()
    
    # Tr√≠ch xu·∫•t keywords ch√≠nh t·ª´ c√¢u h·ªèi
    query_keywords = set(re.findall(r'\b\w+\b', query_lower))
    stop_words = {'c√≥', 'bao', 'nhi√™u', 'l√†', 'g√¨', 'c·ªßa', 'v√†', 'v·ªõi', 't·∫°i', 'v·ªÅ', 'cho', 'ƒë∆∞·ª£c', 'ƒë√£', 's·∫Ω'}
    important_keywords = [w for w in query_keywords if w not in stop_words and len(w) > 2]
    
    # T·ªïng h·ª£p th√¥ng tin t·ª´ c√°c ngu·ªìn
    synthesized_info = []
    extracted_numbers = []
    extracted_facts = []
    
    for i, source in enumerate(sources, 1):
        title = source.get('title', '')
        snippet = source.get('snippet', '')
        full_content = source.get('full_content', '')
        url = source.get('url', '')
        
        # D√πng full_content n·∫øu c√≥, kh√¥ng th√¨ d√πng snippet
        content = full_content if full_content else snippet
        
        # Tr√≠ch xu·∫•t s·ªë li·ªáu li√™n quan (c·∫£i thi·ªán patterns)
        # T√¨m s·ªë trong context (v√≠ d·ª•: "5 qu·∫≠n", "30 qu·∫≠n", "12 qu·∫≠n huy·ªán", "331.212 km¬≤")
        number_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*(?:qu·∫≠n|huy·ªán|t·ªânh|th√†nh ph·ªë|ph∆∞·ªùng|x√£|km¬≤|km2|m¬≤|m2|tri·ªáu|t·ª∑|t·ªâ|ng∆∞·ªùi|d√¢n)',
            r'(?:c√≥|g·ªìm|bao g·ªìm|t·ªïng|t·ªïng c·ªông)\s*(\d+(?:[.,]\d+)?)',
            r'(\d+(?:[.,]\d+)?)\s*(?:ƒë∆°n v·ªã|ƒë·ªãa ph∆∞∆°ng|ng∆∞·ªùi|d√¢n s·ªë)',
            r'(\d+(?:[.,]\d+)?)\s*(?:tri·ªáu|t·ª∑|t·ªâ)\s*(?:ƒë·ªìng|VND|USD)',
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            extracted_numbers.extend(matches)
        
        # Tr√≠ch xu·∫•t c√¢u ch·ª©a keyword quan tr·ªçng (c·∫£i thi·ªán logic)
        sentences = re.split(r'[.!?]\s+', content)
        for sentence in sentences:
            sentence_lower = sentence.lower()
            # Ki·ªÉm tra xem c√¢u c√≥ ch·ª©a keyword quan tr·ªçng kh√¥ng
            keyword_matches = sum(1 for kw in important_keywords[:3] if kw in sentence_lower)
            
            # ∆Øu ti√™n c√¢u c√≥ nhi·ªÅu keyword v√† c√≥ s·ªë li·ªáu
            if keyword_matches > 0:
                has_number = re.search(r'\d+', sentence)
                # N·∫øu c√≥ s·ªë ho·∫∑c c√≥ nhi·ªÅu keyword ‚Üí c√¢u quan tr·ªçng
                if has_number or keyword_matches >= 2:
                    extracted_facts.append(sentence.strip())
                    if len(extracted_facts) >= 5:  # TƒÉng l√™n 5 c√¢u ƒë·ªÉ c√≥ nhi·ªÅu th√¥ng tin h∆°n
                        break
        
        # L∆∞u th√¥ng tin ngu·ªìn
        synthesized_info.append({
            'source_num': i,
            'title': title,
            'url': url,
            'content': content[:500],  # Gi·ªõi h·∫°n 500 chars m·ªói ngu·ªìn
            'has_full_content': bool(full_content)
        })
    
    # T·∫°o context t·ªïng h·ª£p
    context = "\n\n=== TH√îNG TIN T√åM ƒê∆Ø·ª¢C TR√äN INTERNET ===\n\n"
    
    # Ph·∫ßn 1: S·ªë li·ªáu ƒë√£ tr√≠ch xu·∫•t (n·∫øu c√≥)
    if extracted_numbers:
        unique_numbers = list(set(extracted_numbers))
        context += f"üìä S·ªê LI·ªÜU TR√çCH XU·∫§T: {', '.join(unique_numbers)}\n\n"
    
    # Ph·∫ßn 2: C√°c c√¢u quan tr·ªçng
    if extracted_facts:
        context += "üìù TH√îNG TIN QUAN TR·ªåNG:\n"
        for fact in extracted_facts[:3]:
            context += f"- {fact}\n"
        context += "\n"
    
    # Ph·∫ßn 3: Chi ti·∫øt t·ª´ c√°c ngu·ªìn
    context += f"üìö CHI TI·∫æT T·ª™ {len(sources)} D·ªÆ LI·ªÜU THAM KH·∫¢O:\n\n"
    for info in synthesized_info:
        context += f"[D·ªØ li·ªáu {info['source_num']}] {info['title']}\n"
        if info['has_full_content']:
            context += f"üìÑ N·ªôi dung ƒë·∫ßy ƒë·ªß:\n{info['content']}\n\n"
        else:
            context += f"üìÑ Snippet:\n{info['content']}\n\n"
    
    context += "‚ö†Ô∏è Y√äU C·∫¶U TR·∫¢ L·ªúI (Gi·ªëng ChatGPT):\n"
    context += "1. Ph√¢n t√≠ch T·∫§T C·∫¢ c√°c ngu·ªìn tr√™n, so s√°nh v√† cross-check s·ªë li·ªáu\n"
    context += "2. Ch·ªçn th√¥ng tin CH√çNH X√ÅC v√† UY T√çN nh·∫•t (∆∞u ti√™n Wikipedia, b√°o ch√≠nh th·ªëng)\n"
    context += "3. Tr·∫£ l·ªùi T·ª∞ NHI√äN, nh∆∞ th·ªÉ b·∫°n bi·∫øt s·∫µn th√¥ng tin ƒë√≥\n"
    context += "4. KH√îNG ƒë·ªÅ c·∫≠p ƒë·∫øn 'ngu·ªìn', 't√¨m ƒë∆∞·ª£c', 'theo...', 'd·ª±a v√†o', 'd·ªØ li·ªáu', 'tham kh·∫£o'\n"
    context += "5. Tr√°nh k·ªÉ t√™n c√°c t√†i li·ªáu ho·∫∑c website trong c√¢u tr·∫£ l·ªùi cu·ªëi\n"
    context += "6. N·∫øu c√≥ m√¢u thu·∫´n gi·ªØa c√°c ngu·ªìn, ch·ªçn s·ªë li·ªáu xu·∫•t hi·ªán nhi·ªÅu nh·∫•t ho·∫∑c t·ª´ ngu·ªìn uy t√≠n nh·∫•t\n"
    context += "7. CH·ªà tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n, d·ªÖ hi·ªÉu\n"
    context += "8. Tr·∫£ l·ªùi b·∫±ng 1 c√¢u duy nh·∫•t (t·ªëi ƒëa 25 t·ª´), ƒëi th·∫≥ng v√†o th√¥ng tin ch√≠nh\n"
    context += "9. N·∫øu c·∫ßn li·ªát k√™, ch·ªâ li·ªát k√™ ng·∫Øn g·ªçn trong c√πng m·ªôt c√¢u, tr√°nh xu·ªëng d√≤ng\n"
    context += "10. KH√îNG l·∫∑p l·∫°i y√™u c·∫ßu, KH√îNG gi·∫£i th√≠ch quy tr√¨nh\n\n"
    
    return context


def get_accuweather_forecast(location_key: str = "353412", city_name: str = "Hanoi") -> dict:
    """
    L·∫•y d·ª± b√°o th·ªùi ti·∫øt t·ª´ AccuWeather API
    
    Args:
        location_key: AccuWeather location key (ti·∫øt ki·ªám API calls)
        city_name: T√™n th√†nh ph·ªë ƒë·ªÉ hi·ªÉn th·ªã
    
    Returns:
        Dict v·ªõi forecast ho·∫∑c error
    """
    if not ACCUWEATHER_API_KEY:
        logging.error("‚ùå AccuWeather API key not found")
        return {'error': 'Ch∆∞a c·∫•u h√¨nh AccuWeather API key'}
    
    try:
        logging.info(f"üå§Ô∏è Getting weather forecast for {city_name} (key={location_key})")
        
        # L·∫•y d·ª± b√°o 1 ng√†y (b·ªè qua b∆∞·ªõc search location)
        forecast_url = f"http://dataservice.accuweather.com/forecasts/v1/daily/1day/{location_key}"
        forecast_params = {
            'apikey': ACCUWEATHER_API_KEY,
            'language': 'vi-vn',
            'details': 'true',
            'metric': 'true'
        }
        
        forecast_resp = requests.get(forecast_url, params=forecast_params, timeout=10)
        forecast_resp.raise_for_status()
        forecast_data = forecast_resp.json()
        
        # Parse d·ªØ li·ªáu
        daily_forecast = forecast_data['DailyForecasts'][0]
        
        result = {
            'city': city_name,
            'date': daily_forecast['Date'],
            'temperature_min': daily_forecast['Temperature']['Minimum']['Value'],
            'temperature_max': daily_forecast['Temperature']['Maximum']['Value'],
            'day_condition': daily_forecast['Day']['IconPhrase'],
            'night_condition': daily_forecast['Night']['IconPhrase'],
            'rain_probability': daily_forecast['Day'].get('RainProbability', 0),
            'headline': forecast_data['Headline']['Text']
        }
        
        logging.info(f"‚úÖ Weather forecast: {result['temperature_min']}-{result['temperature_max']}¬∞C, {result['day_condition']}")
        
        return result
        
    except Exception as e:
        logging.error(f"‚ùå Error getting weather: {e}")
        return {'error': f'L·ªói l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt: {str(e)}'}


def get_accuweather_forecast_by_name(location: str = "Hanoi") -> dict:
    """
    L·∫•y d·ª± b√°o th·ªùi ti·∫øt b·∫±ng c√°ch search location (t·ªën 1 API call)
    Ch·ªâ d√πng cho c√°c t·ªânh kh√¥ng c√≥ location_key s·∫µn
    """
    if not ACCUWEATHER_API_KEY:
        return {'error': 'Ch∆∞a c·∫•u h√¨nh AccuWeather API key'}
    
    try:
        # 1. Search location
        location_url = f"http://dataservice.accuweather.com/locations/v1/cities/search"
        location_params = {
            'apikey': ACCUWEATHER_API_KEY,
            'q': location,
            'language': 'vi-vn'
        }
        
        location_resp = requests.get(location_url, params=location_params, timeout=10)
        location_resp.raise_for_status()
        locations = location_resp.json()
        
        if not locations:
            return {'error': f'Kh√¥ng t√¨m th·∫•y th√†nh ph·ªë {location}'}
        
        location_key = locations[0]['Key']
        city_name = locations[0]['LocalizedName']
        
        # 2. G·ªçi h√†m ch√≠nh v·ªõi location_key
        return get_accuweather_forecast(location_key, city_name)
        
    except Exception as e:
        logging.error(f"‚ùå Error searching location: {e}")
        return {'error': f'L·ªói t√¨m th√†nh ph·ªë: {str(e)}'}


def extract_hour_from_question(question: str) -> int:
    """
    Tr√≠ch xu·∫•t gi·ªù t·ª´ c√¢u h·ªèi (v√≠ d·ª•: "12h", "12 gi·ªù", "bu·ªïi tr∆∞a")
    
    Returns:
        Gi·ªù (0-23) ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    question_lower = question.lower()
    
    # Pattern 1: "12h", "12h00", "12:00"
    hour_match = re.search(r'(\d{1,2})[h:]\d{0,2}', question_lower)
    if hour_match:
        hour = int(hour_match.group(1))
        if 0 <= hour <= 23:
            return hour
    
    # Pattern 2: "12 gi·ªù"
    hour_match = re.search(r'(\d{1,2})\s*gi·ªù', question_lower)
    if hour_match:
        hour = int(hour_match.group(1))
        if 0 <= hour <= 23:
            return hour
    
    # Pattern 3: Bu·ªïi trong ng√†y
    if 's√°ng' in question_lower or 'bu·ªïi s√°ng' in question_lower:
        return 8  # 8h s√°ng
    elif 'tr∆∞a' in question_lower or 'bu·ªïi tr∆∞a' in question_lower:
        return 12  # 12h tr∆∞a
    elif 'chi·ªÅu' in question_lower or 'bu·ªïi chi·ªÅu' in question_lower:
        return 15  # 15h chi·ªÅu
    elif 't·ªëi' in question_lower or 'bu·ªïi t·ªëi' in question_lower:
        return 19  # 19h t·ªëi
    elif 'ƒë√™m' in question_lower or 'bu·ªïi ƒë√™m' in question_lower:
        return 22  # 22h ƒë√™m
    
    return None


def get_weather_chatgpt_style(city_name: str, question: str) -> dict:
    """
    üåê QUY TR√åNH CHATGPT: L·∫•y th·ªùi ti·∫øt t·ª´ ACCUWEATHER + TR√çCH XU·∫§T TH√îNG MINH
    
    B∆∞·ªõc 1: Web Search - T√¨m AccuWeather cho th√†nh ph·ªë
    B∆∞·ªõc 2: Tr√≠ch xu·∫•t d·ªØ li·ªáu chi ti·∫øt - Nhi·ªát ƒë·ªô theo gi·ªù, ƒëi·ªÅu ki·ªán, ƒë·ªô ·∫©m
    B∆∞·ªõc 3: X·ª≠ l√Ω c√¢u h·ªèi theo gi·ªù c·ª• th·ªÉ (n·∫øu c√≥)
    B∆∞·ªõc 4: Chu·∫©n h√≥a - Format d·ªØ li·ªáu th√¢n thi·ªán ti·∫øng Vi·ªát
    
    Args:
        city_name: T√™n th√†nh ph·ªë (H√† N·ªôi, S√†i G√≤n, ƒê√† N·∫µng...)
        question: C√¢u h·ªèi g·ªëc c·ªßa user
    
    Returns:
        Dict v·ªõi th√¥ng tin th·ªùi ti·∫øt ƒë·∫ßy ƒë·ªß ho·∫∑c error
    """
    try:
        logging.info(f"üß† [ChatGPT Style] Analyzing weather query for: {city_name}")
        
        # Ph√°t hi·ªán c√¢u h·ªèi v·ªÅ th·ªùi ti·∫øt theo gi·ªù
        target_hour = extract_hour_from_question(question)
        if target_hour is not None:
            logging.info(f"‚è∞ Detected hour-specific query: {target_hour}h")
        
        # ===== B∆Ø·ªöC 1: WEB SEARCH - T√åM ACCUWEATHER =====
        # T·∫°o query th√¥ng minh - ∆∞u ti√™n AccuWeather
        search_query = f"th·ªùi ti·∫øt {city_name} h√¥m nay accuweather"
        
        logging.info(f"üîç Step 1: Web search - '{search_query}'")
        
        ddgs = DDGS()
        results = list(ddgs.text(
            search_query,
            region='vn-vi',
            safesearch='moderate',
            max_results=10
        ))
        
        if not results:
            logging.warning("‚ö†Ô∏è No web results found")
            return {'error': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin th·ªùi ti·∫øt'}
        
        logging.info(f"üì• Found {len(results)} sources")
        
        # ∆Øu ti√™n AccuWeather URL
        accuweather_url = None
        for result in results:
            url = result.get('href', '')
            if 'accuweather.com' in url.lower():
                accuweather_url = url
                logging.info(f"‚úÖ Found AccuWeather URL: {url[:80]}")
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y AccuWeather, d√πng ngu·ªìn ƒë·∫ßu ti√™n
        if not accuweather_url and results:
            accuweather_url = results[0].get('href', '')
            logging.info(f"‚ö†Ô∏è AccuWeather not found, using first result: {accuweather_url[:80]}")
        
        if not accuweather_url:
            return {'error': 'Kh√¥ng t√¨m th·∫•y trang AccuWeather'}
        
        # ===== B∆Ø·ªöC 2: TR√çCH XU·∫§T D·ªÆ LI·ªÜU T·ª™ ACCUWEATHER =====
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
            }
            
            response = requests.get(accuweather_url, headers=headers, timeout=10, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            text_content = soup.get_text()
            
            # TR√çCH XU·∫§T: Nhi·ªát ƒë·ªô hi·ªán t·∫°i
            current_temp_patterns = [
                r'(\d{1,2})[¬∞\s]*C\s*(?:hi·ªán t·∫°i|now|current)',
                r'(?:hi·ªán t·∫°i|now|current)[^\d]*(\d{1,2})[¬∞\s]*C',
                r'(\d{1,2})[¬∞\s]*C\s*(?:¬∞|degrees)'
            ]
            current_temp = None
            for pattern in current_temp_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    temp_val = int(match.group(1))
                    if -10 <= temp_val <= 50:  # Nhi·ªát ƒë·ªô h·ª£p l√Ω
                        current_temp = temp_val
                        break
            
            # TR√çCH XU·∫§T: Nhi·ªát ƒë·ªô min-max
            temp_range_pattern = r'(\d{1,2})[¬∞\s]*-[¬∞\s]*(\d{1,2})[¬∞\s]*C'
            temp_match = re.search(temp_range_pattern, text_content)
            temp_min = None
            temp_max = None
            if temp_match:
                temp_min = int(temp_match.group(1))
                temp_max = int(temp_match.group(2))
            else:
                # T√¨m ri√™ng min v√† max
                min_match = re.search(r'(?:min|t·ªëi thi·ªÉu|th·∫•p nh·∫•t)[^\d]*(\d{1,2})[¬∞\s]*C', text_content, re.IGNORECASE)
                max_match = re.search(r'(?:max|t·ªëi ƒëa|cao nh·∫•t)[^\d]*(\d{1,2})[¬∞\s]*C', text_content, re.IGNORECASE)
                if min_match:
                    temp_min = int(min_match.group(1))
                if max_match:
                    temp_max = int(max_match.group(1))
            
            # TR√çCH XU·∫§T: ƒêi·ªÅu ki·ªán th·ªùi ti·∫øt
            conditions = []
            condition_keywords = {
                'n·∫Øng': ['sunny', 'n·∫Øng', 'quang ƒë√£ng', 'clear'],
                'nhi·ªÅu m√¢y': ['cloudy', 'nhi·ªÅu m√¢y', 'c√≥ m√¢y', 'overcast'],
                'm∆∞a': ['rain', 'm∆∞a', 'rainy', 'drizzle'],
                'm∆∞a to': ['heavy rain', 'm∆∞a to', 'downpour'],
                's∆∞∆°ng m√π': ['fog', 's∆∞∆°ng m√π', 'mist', 'haze'],
                'gi√≥': ['windy', 'gi√≥', 'breeze']
            }
            
            text_lower = text_content.lower()
            for condition, keywords in condition_keywords.items():
                if any(kw in text_lower for kw in keywords):
                    conditions.append(condition)
            
            # TR√çCH XU·∫§T: ƒê·ªô ·∫©m
            humidity_pattern = r'(?:ƒë·ªô ·∫©m|humidity)[^\d]*(\d{2,3})%'
            humidity_match = re.search(humidity_pattern, text_content, re.IGNORECASE)
            humidity = int(humidity_match.group(1)) if humidity_match else None
            
            # TR√çCH XU·∫§T: Nhi·ªát ƒë·ªô theo gi·ªù (n·∫øu c√≥ c√¢u h·ªèi v·ªÅ gi·ªù c·ª• th·ªÉ)
            hourly_data = {}
            if target_hour is not None:
                # T√¨m b·∫£ng hourly forecast ho·∫∑c th√¥ng tin theo gi·ªù
                # Pattern: "12h: 25¬∞C" ho·∫∑c "12:00 25¬∞C"
                hour_pattern = rf'{target_hour}[h:]\d{{0,2}}[^\d]*(\d{{1,2}})[¬∞\s]*C'
                hour_match = re.search(hour_pattern, text_content, re.IGNORECASE)
                if hour_match:
                    hourly_data[target_hour] = int(hour_match.group(1))
                    logging.info(f"‚úÖ Found temperature at {target_hour}h: {hourly_data[target_hour]}¬∞C")
            
            # T·∫°o k·∫øt qu·∫£
            result = {
                'city': city_name,
                'current_temperature': current_temp,
                'temperature_min': temp_min,
                'temperature_max': temp_max,
                'humidity': humidity,
                'conditions': ', '.join(conditions[:3]) if conditions else 'Ch∆∞a r√µ',
                'source': 'accuweather',
                'url': accuweather_url
            }
            
            # Th√™m th√¥ng tin theo gi·ªù n·∫øu c√≥
            if target_hour is not None:
                if target_hour in hourly_data:
                    result['hourly_temperature'] = hourly_data[target_hour]
                    result['target_hour'] = target_hour
                else:
                    # ∆Ø·ªõc t√≠nh nhi·ªát ƒë·ªô theo gi·ªù d·ª±a tr√™n min-max
                    if temp_min and temp_max:
                        # Gi·∫£ s·ª≠ nhi·ªát ƒë·ªô th·∫•p nh·∫•t v√†o s√°ng s·ªõm (6h), cao nh·∫•t v√†o chi·ªÅu (14h)
                        if 6 <= target_hour <= 14:
                            # TƒÉng d·∫ßn t·ª´ s√°ng ƒë·∫øn chi·ªÅu
                            progress = (target_hour - 6) / 8  # 0-1
                            estimated = temp_min + (temp_max - temp_min) * progress
                        else:
                            # Gi·∫£m d·∫ßn t·ª´ chi·ªÅu ƒë·∫øn ƒë√™m
                            if target_hour > 14:
                                progress = (24 - target_hour + 14) / 16  # Gi·∫£m d·∫ßn
                            else:
                                progress = (target_hour + 10) / 16  # ƒê√™m ƒë·∫øn s√°ng
                            estimated = temp_max - (temp_max - temp_min) * progress
                        
                        result['hourly_temperature'] = round(estimated)
                        result['target_hour'] = target_hour
                        result['estimated'] = True
                        logging.info(f"üìä Estimated temperature at {target_hour}h: {result['hourly_temperature']}¬∞C")
            
            logging.info(f"‚úÖ Extracted weather data: {result.get('current_temperature') or f'{temp_min}-{temp_max}'}¬∞C, {result['conditions']}")
            return result
            
        except Exception as e:
            logging.error(f"‚ùå Error extracting from AccuWeather: {e}")
            return {'error': f'L·ªói tr√≠ch xu·∫•t d·ªØ li·ªáu: {str(e)}'}
        
    except Exception as e:
        logging.error(f"‚ùå Error in ChatGPT-style weather: {e}")
        logging.error(traceback.format_exc())
        return {'error': f'L·ªói l·∫•y th·ªùi ti·∫øt: {str(e)}'}


def optimize_query_for_wikipedia(query: str) -> str:
    """
    T·ªëi ∆∞u query cho Wikipedia API - lo·∫°i b·ªè t·ª´ kh√¥ng c·∫ßn thi·∫øt, gi·ªØ keywords ch√≠nh
    QUAN TR·ªåNG: Gi·ªØ nguy√™n c√°c c·ª•m t·ª´ ƒë·ªãa danh/t√™n ri√™ng (h√† n·ªôi, s√†i g√≤n, vi·ªát nam...)
    
    Args:
        query: C√¢u h·ªèi g·ªëc
    
    Returns:
        Query ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u
    """
    query_lower = query.lower()
    
    # Danh s√°ch c·ª•m t·ª´ ƒë·ªãa danh/t√™n ri√™ng quan tr·ªçng (KH√îNG ƒë∆∞·ª£c t√°ch)
    proper_nouns = [
        'h√† n·ªôi', 'hanoi', 's√†i g√≤n', 'saigon', 'h·ªì ch√≠ minh', 'ho chi minh',
        'vi·ªát nam', 'vietnam', 'ƒë√† n·∫µng', 'da nang', 'c·∫ßn th∆°', 'can tho',
        'h·∫£i ph√≤ng', 'hai phong', 'ngh·ªá an', 'nghe an', 'thanh h√≥a', 'thanh hoa',
        'qu·∫£ng ninh', 'quang ninh', 'h·∫° long', 'ha long', 'hu·∫ø', 'hue',
        'nha trang', 'ƒë√† l·∫°t', 'da lat', 'v≈©ng t√†u', 'vung tau'
    ]
    
    # T√¨m v√† gi·ªØ nguy√™n c√°c c·ª•m t·ª´ ƒë·ªãa danh
    found_proper_nouns = []
    remaining_query = query_lower
    
    for pn in proper_nouns:
        if pn in remaining_query:
            found_proper_nouns.append(pn)
            # Lo·∫°i b·ªè c·ª•m t·ª´ n√†y kh·ªèi query ƒë·ªÉ kh√¥ng b·ªã x·ª≠ l√Ω l·∫°i
            remaining_query = remaining_query.replace(pn, '')
    
    # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt t·ª´ ph·∫ßn c√≤n l·∫°i
    stop_words = {
        'c√≥', 'bao', 'nhi√™u', 'l√†', 'g√¨', 'c·ªßa', 'v√†', 'v·ªõi', 't·∫°i', 'v·ªÅ', 'cho', 
        'ƒë∆∞·ª£c', 'ƒë√£', 's·∫Ω', 'b·∫°n', 't√¥i', 't√¨m', 'chi', 'ti·∫øt', 'th√¥ng', 'tin'
    }
    
    # T√°ch t·ª´ v√† lo·∫°i b·ªè stop words t·ª´ ph·∫ßn c√≤n l·∫°i
    words = remaining_query.split()
    keywords = [w for w in words if w not in stop_words and len(w) > 2]
    
    # K·∫øt h·ª£p: c·ª•m t·ª´ ƒë·ªãa danh + keywords c√≤n l·∫°i
    all_keywords = found_proper_nouns + keywords
    
    # Gi·ªõi h·∫°n t·ªëi ƒëa 6 t·ª´ (tƒÉng l√™n ƒë·ªÉ gi·ªØ ƒë·ªß th√¥ng tin)
    optimized = ' '.join(all_keywords[:6])
    
    # N·∫øu sau khi optimize qu√° ng·∫Øn ho·∫∑c m·∫•t h·∫øt th√¥ng tin ‚Üí d√πng query g·ªëc
    if not optimized or len(optimized.split()) < 2:
        logging.warning(f"‚ö†Ô∏è Query optimization qu√° m·∫°nh, d√πng query g·ªëc: '{query}'")
        return query
    
    return optimized


def search_wikipedia_api(query: str) -> list:
    """
    T√¨m ki·∫øm tr·ª±c ti·∫øp t·ª´ Wikipedia API - K·∫øt qu·∫£ r·∫•t t·ªët v√† uy t√≠n
    
    Args:
        query: C√¢u h·ªèi t√¨m ki·∫øm
    
    Returns:
        List of results ho·∫∑c empty list
    """
    try:
        logging.info(f"üìö Wikipedia API Search: '{query}'")
        
        # T·ªëi ∆∞u query tr∆∞·ªõc khi search
        optimized_query = optimize_query_for_wikipedia(query)
        if optimized_query != query:
            logging.info(f"   üîß Optimized query: '{query}' ‚Üí '{optimized_query}'")
        
        # Wikipedia OpenSearch API - kh√¥ng c·∫ßn API key
        # QUAN TR·ªåNG: Wikipedia y√™u c·∫ßu User-Agent h·ª£p l·ªá
        wiki_url = "https://vi.wikipedia.org/w/api.php"
        params = {
            'action': 'opensearch',
            'search': optimized_query,  # D√πng query ƒë√£ t·ªëi ∆∞u
            'limit': 3,  # L·∫•y 3 k·∫øt qu·∫£ t·ªët nh·∫•t
            'namespace': 0,  # Ch·ªâ t√¨m trong namespace ch√≠nh
            'profile': 'fuzzy',
            'format': 'json'
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json'
        }
        
        response = requests.get(wiki_url, params=params, headers=headers, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        # Format: [query, [titles], [descriptions], [urls]]
        if len(data) >= 4 and len(data[1]) > 0:
            titles = data[1]
            descriptions = data[2] if len(data) > 2 else []
            urls = data[3] if len(data) > 3 else []
            
            wiki_results = []
            for i, title in enumerate(titles):
                wiki_results.append({
                    'title': title,
                    'snippet': descriptions[i] if i < len(descriptions) else '',
                    'url': urls[i] if i < len(urls) else '',
                    'source': 'wikipedia_api',
                    'relevance_score': 1.0  # Wikipedia lu√¥n c√≥ ƒëi·ªÉm cao nh·∫•t
                })
            
            # L·ªçc k·∫øt qu·∫£ kh√¥ng kh·ªõp v·ªõi keyword quan tr·ªçng trong query (tr√°nh nh·∫ßm H√† Nam, H√† T√¢y, ...)
            optimized_keywords = [kw for kw in optimized_query.lower().split() if len(kw) > 2]
            if not optimized_keywords:
                optimized_keywords = [kw for kw in query.lower().split() if len(kw) > 2]
            
            filtered_results = []
            for result in wiki_results:
                title_lower = result['title'].lower()
                snippet_lower = result['snippet'].lower()
                
                if optimized_keywords and not any(kw in title_lower or kw in snippet_lower for kw in optimized_keywords):
                    logging.info(f"   ‚úó B·ªè '{result['title'][:60]}' (kh√¥ng kh·ªõp keyword quan tr·ªçng: {optimized_keywords[:3]})")
                    continue
                filtered_results.append(result)
            
            if filtered_results and len(filtered_results) != len(wiki_results):
                logging.info(f"   üîé Sau khi l·ªçc c√≤n {len(filtered_results)} k·∫øt qu·∫£ ph√π h·ª£p t·ª´ Wikipedia API")
            elif not filtered_results:
                logging.info(f"‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£ Wikipedia ph√π h·ª£p v·ªõi keyword sau khi l·ªçc (query '{query}')")
                return []
            
            logging.info(f"‚úÖ T√¨m th·∫•y {len(filtered_results)} k·∫øt qu·∫£ t·ª´ Wikipedia API")
            for i, result in enumerate(filtered_results, 1):
                logging.info(f"   [{i}] {result['title'][:60]}...")
            return filtered_results
        else:
            # N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ v·ªõi query ƒë√£ optimize, th·ª≠ l·∫°i v·ªõi query g·ªëc (n·∫øu kh√°c)
            if optimized_query != query:
                logging.info(f"‚ö†Ô∏è Wikipedia API kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ v·ªõi query ƒë√£ optimize '{optimized_query}', th·ª≠ l·∫°i v·ªõi query g·ªëc '{query}'...")
                # Th·ª≠ l·∫°i v·ªõi query g·ªëc (ch·ªâ l·∫ßn ƒë·∫ßu, tr√°nh loop v√¥ h·∫°n)
                params['search'] = query
                try:
                    response_retry = requests.get(wiki_url, params=params, headers=headers, timeout=5)
                    response_retry.raise_for_status()
                    data_retry = response_retry.json()
                    
                    if len(data_retry) >= 4 and len(data_retry[1]) > 0:
                        titles = data_retry[1]
                        descriptions = data_retry[2] if len(data_retry) > 2 else []
                        urls = data_retry[3] if len(data_retry) > 3 else []
                        
                        wiki_results = []
                        for i, title in enumerate(titles):
                            wiki_results.append({
                                'title': title,
                                'snippet': descriptions[i] if i < len(descriptions) else '',
                                'url': urls[i] if i < len(urls) else '',
                                'source': 'wikipedia_api',
                                'relevance_score': 1.0
                            })
                        
                        filtered_results = []
                        for result in wiki_results:
                            title_lower = result['title'].lower()
                            snippet_lower = result['snippet'].lower()
                            
                            if optimized_keywords and not any(kw in title_lower or kw in snippet_lower for kw in optimized_keywords):
                                logging.info(f"   ‚úó B·ªè '{result['title'][:60]}' (retry - kh√¥ng kh·ªõp keyword quan tr·ªçng: {optimized_keywords[:3]})")
                                continue
                            filtered_results.append(result)
                        
                        if filtered_results and len(filtered_results) != len(wiki_results):
                            logging.info(f"   üîé Sau khi l·ªçc c√≤n {len(filtered_results)} k·∫øt qu·∫£ ph√π h·ª£p t·ª´ Wikipedia API (query g·ªëc)")
                        elif not filtered_results:
                            logging.info(f"‚ö†Ô∏è Wikipedia API (query g·ªëc) kh√¥ng c√≥ k·∫øt qu·∫£ ph√π h·ª£p v·ªõi keyword, s·∫Ω d√πng DuckDuckGo")
                            return []
                        
                        logging.info(f"‚úÖ T√¨m th·∫•y {len(filtered_results)} k·∫øt qu·∫£ t·ª´ Wikipedia API (v·ªõi query g·ªëc)")
                        return filtered_results
                    else:
                        logging.info(f"‚ö†Ô∏è Wikipedia API kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ c·∫£ v·ªõi query g·ªëc '{query}' (s·∫Ω d√πng DuckDuckGo)")
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è L·ªói khi retry Wikipedia API v·ªõi query g·ªëc: {e}")
            else:
                logging.info(f"‚ö†Ô∏è Wikipedia API kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ cho query '{query}' (s·∫Ω d√πng DuckDuckGo)")
            
            return []
        
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Wikipedia API error: {e}")
        return []


def search_bing_api(query: str, max_results: int = 5) -> list:
    """
    T√¨m ki·∫øm t·ª´ Bing Search API - K·∫øt qu·∫£ r·∫•t t·ªët v√† ch√≠nh x√°c
    
    Args:
        query: C√¢u h·ªèi t√¨m ki·∫øm
        max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
    
    Returns:
        List of results ho·∫∑c empty list
    """
    if not BING_SEARCH_API_KEY:
        logging.warning("‚ö†Ô∏è Bing Search API key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, b·ªè qua Bing search")
        return []
    
    try:
        logging.info(f"üîç Bing Search API: '{query}'")
        
        headers = {
            'Ocp-Apim-Subscription-Key': BING_SEARCH_API_KEY,
            'Accept': 'application/json'
        }
        
        params = {
            'q': query,
            'count': min(max_results, 10),  # Bing API t·ªëi ƒëa 50, nh∆∞ng d√πng 10 ƒë·ªÉ nhanh
            'offset': 0,
            'mkt': 'vi-VN',  # Market: Vietnam
            'safeSearch': 'Moderate',
            'responseFilter': 'Webpages'  # Ch·ªâ l·∫•y webpages, kh√¥ng l·∫•y images/videos
        }
        
        response = requests.get(BING_SEARCH_ENDPOINT, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        
        # Parse k·∫øt qu·∫£ t·ª´ Bing API
        if 'webPages' in data and 'value' in data['webPages']:
            bing_results = []
            for item in data['webPages']['value']:
                bing_results.append({
                    'title': item.get('name', ''),
                    'snippet': item.get('snippet', ''),
                    'url': item.get('url', ''),
                    'source': 'bing_api',
                    'relevance_score': 0.9  # Bing API c√≥ ƒë·ªô ch√≠nh x√°c cao
                })
            
            logging.info(f"‚úÖ T√¨m th·∫•y {len(bing_results)} k·∫øt qu·∫£ t·ª´ Bing Search API")
            for i, result in enumerate(bing_results, 1):
                logging.info(f"   [{i}] {result['title'][:60]}...")
            return bing_results
        else:
            logging.info(f"‚ö†Ô∏è Bing Search API kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
            return []
        
    except requests.exceptions.RequestException as e:
        logging.warning(f"‚ö†Ô∏è Bing Search API error (network): {e}")
        return []
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Bing Search API error: {e}")
        return []


def search_web(query: str, max_results: int = 5, prioritize_today: bool = False) -> dict:
    """
    T√¨m ki·∫øm web v·ªõi nhi·ªÅu ngu·ªìn: Wikipedia API + DuckDuckGo - L·∫•y 3 ngu·ªìn t·ªët nh·∫•t (ChatGPT style)
    
    Args:
        query: C√¢u h·ªèi t√¨m ki·∫øm
        max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa
        prioritize_today: N·∫øu True, ∆∞u ti√™n k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i (cho gi√° v√†ng h√¥m nay)
    
    Returns:
        Dict v·ªõi results ho·∫∑c error (list of 3 sources)
    """
    try:
        logging.info(f"üîç Web Search: '{query}'")
        
        all_results = []
        
        # ===== B∆Ø·ªöC 1: T√¨m ki·∫øm Wikipedia API tr∆∞·ªõc (∆∞u ti√™n cao nh·∫•t) =====
        wiki_results = search_wikipedia_api(query)
        if wiki_results:
            all_results.extend(wiki_results)
            logging.info(f"üìö ƒê√£ th√™m {len(wiki_results)} k·∫øt qu·∫£ t·ª´ Wikipedia API")
        
        # ===== B∆Ø·ªöC 1.5: T√¨m ki·∫øm Bing Search API (n·∫øu c√≥ API key) - K·∫øt qu·∫£ r·∫•t t·ªët =====
        bing_results = search_bing_api(query, max_results=5)
        if bing_results:
            all_results.extend(bing_results)
            logging.info(f"üîç ƒê√£ th√™m {len(bing_results)} k·∫øt qu·∫£ t·ª´ Bing Search API")
        
        # ===== B∆Ø·ªöC 2: T√¨m ki·∫øm DuckDuckGo ƒë·ªÉ b·ªï sung =====
        ddgs = DDGS()
        
        # T√¨m ki·∫øm CH√çNH X√ÅC nh∆∞ DuckDuckGo web - KH√îNG d√πng timelimit
        results_iter = ddgs.text(
            query,  # D√πng query g·ªëc
            region='vn-vi',
            safesearch='moderate',
            # B·ªé timelimit ƒë·ªÉ k·∫øt qu·∫£ gi·ªëng web DuckDuckGo
            max_results=20  # T√¨m nhi·ªÅu ƒë·ªÉ ch·ªçn ngu·ªìn uy t√≠n
        )
        
        # Convert iterator sang list
        ddg_results = list(results_iter)
        
        if ddg_results:
            logging.info(f"üì• Nh·∫≠n ƒë∆∞·ª£c {len(ddg_results)} k·∫øt qu·∫£ t·ª´ DuckDuckGo")
            all_results.extend(ddg_results)
        
        if not all_results:
            return {'error': 'Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£'}
        
        logging.info(f"üìä T·ªïng c·ªông: {len(all_results)} k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ ngu·ªìn")
        
        # ∆Øu ti√™n ngu·ªìn M·ªöI v√† UY T√çN
        formatted_results = []
        
        # Danh s√°ch domain uy t√≠n (∆∞u ti√™n cao)
        trusted_domains = [
            # Wikipedia - ∆Øu ti√™n s·ªë 1
            'wikipedia.org', 'vi.wikipedia.org', 'en.wikipedia.org',
            # B√°o ch√≠ uy t√≠n
            'vnexpress.net', 'dantri.com.vn', 'tuoitre.vn', 'tuoitrenews.vn',
            'thanhnien.vn', 'nguoiduatin.vn', 'baomoi.com', 'vietnamnet.vn',
            # T√†i ch√≠nh
            'cafef.vn', 'vneconomy.vn', 'ndh.vn', 'bnews.vn',
            # Ch√≠nh ph·ªß
            'baochinhphu.vn', 'gov.vn', 'vn.gov.vn', 'gso.gov.vn',
            # Kh√°c
            'sggp.org.vn',
            # Th·ªùi ti·∫øt
            'nchmf.gov.vn', 'accuweather.com', 'weather.com'
        ]
        
        # Danh s√°ch domain B·ªé QUA (kh√¥ng chu·∫©n)
        blocked_domains = [
            'mojeek.com', 'www.mojeek.com', 'mojeek.vn',  # Mojeek kh√¥ng chu·∫©n - ch·ªâ l√† search engine, kh√¥ng ph·∫£i ngu·ªìn tin
            'search.mojeek.com'
        ]
        
        # Danh s√°ch t·ª´ kh√≥a B·ªé QUA (kh√¥ng li√™n quan ƒë·∫øn gi√° v√†ng)
        irrelevant_keywords = [
            't·ª≠ vi', 't·ª≠ vi h√¥m nay', 'xem t·ª≠ vi', '12 con gi√°p',
            'l·ªãch √¢m', '√¢m l·ªãch', 'l·ªãch v·∫°n ni√™n', 'ng√†y t·ªët',
            'phong th·ªßy', 'b√≥i to√°n', 'chi√™m tinh',
            'gi·∫£i tr√≠', 'tin t·ª©c gi·∫£i tr√≠', 'showbiz'
        ]
        
        # ===== CHI·∫æN L∆Ø·ª¢C M·ªöI: L·ªçc theo ƒë·ªô li√™n quan + L·∫•y nhi·ªÅu ngu·ªìn =====
        
        scored_results = []
        blocked_count = 0
        
        # Tr√≠ch xu·∫•t keywords quan tr·ªçng t·ª´ query
        import re
        from datetime import datetime
        query_lower = query.lower()
        
        # Ph√°t hi·ªán y√™u c·∫ßu v·ªÅ ng√†y c·ª• th·ªÉ
        today = datetime.now()
        current_date_str = today.strftime('%d/%m/%Y')
        current_date_str_short = today.strftime('%d/%m')
        current_date_str_dash = today.strftime('%d-%m')
        
        # N·∫øu prioritize_today=True, ∆∞u ti√™n k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i
        has_today = 'h√¥m nay' in query_lower or prioritize_today
        has_date = re.search(r'(\d{1,2})[/-](\d{1,2})', query_lower)  # VD: 11/11, 11-11
        
        # Duy·ªát qua k·∫øt qu·∫£ v√† t√≠nh ƒëi·ªÉm li√™n quan
        for i, result in enumerate(all_results, 1):
            # X·ª≠ l√Ω format kh√°c nhau: Wikipedia API vs Bing API vs DuckDuckGo
            if isinstance(result, dict) and 'source' in result:
                if result['source'] == 'wikipedia_api':
                    # K·∫øt qu·∫£ t·ª´ Wikipedia API
                    title = result.get('title', '')
                    snippet = result.get('snippet', '')
                    url = result.get('url', '')
                    relevance_score = result.get('relevance_score', 1.0)
                    is_wikipedia = True
                elif result['source'] == 'bing_api':
                    # K·∫øt qu·∫£ t·ª´ Bing Search API
                    title = result.get('title', '')
                    snippet = result.get('snippet', '')
                    url = result.get('url', '')
                    relevance_score = result.get('relevance_score', 0.9)  # Bing c√≥ ƒë·ªô ch√≠nh x√°c cao
                    is_wikipedia = 'wikipedia.org' in url
                else:
                    # K·∫øt qu·∫£ t·ª´ ngu·ªìn kh√°c
                    title = result.get('title', '')
                    snippet = result.get('snippet', '') or result.get('body', '')
                    url = result.get('url', '') or result.get('href', '')
                    relevance_score = None
                    is_wikipedia = 'wikipedia.org' in url
            else:
                # K·∫øt qu·∫£ t·ª´ DuckDuckGo (format c≈©)
                title = result.get('title', '')
                snippet = result.get('body', '')
                url = result.get('href', '')
                relevance_score = None  # S·∫Ω t√≠nh sau
                is_wikipedia = 'wikipedia.org' in url
            
            # B·ªè qua domain b·ªã ch·∫∑n (Mojeek, Yahoo, search engines)
            # Ki·ªÉm tra c·∫£ domain ch√≠nh v√† subdomain
            url_lower = url.lower()
            is_blocked = any(domain.lower() in url_lower for domain in blocked_domains)
            if is_blocked:
                logging.info(f"  ‚úó [{i}] BLOCKED | {title[:60]}...")
                blocked_count += 1
                continue
            
            # B·ªè qua k·∫øt qu·∫£ kh√¥ng li√™n quan ƒë·∫øn gi√° v√†ng (t·ª≠ vi, l·ªãch √¢m, etc.)
            title_lower_check = title.lower()
            snippet_lower_check = snippet.lower()
            is_irrelevant = any(kw in title_lower_check or kw in snippet_lower_check for kw in irrelevant_keywords)
            
            # Ch·ªâ b·ªè qua n·∫øu query v·ªÅ gi√° v√†ng
            if ('gi√° v√†ng' in query_lower or 'v√†ng sjc' in query_lower) and is_irrelevant:
                logging.info(f"  ‚úó [{i}] KH√îNG LI√äN QUAN (t·ª≠ vi/l·ªãch √¢m) | {title[:60]}...")
                blocked_count += 1
                continue
            
            # T√≠nh ƒëi·ªÉm li√™n quan (n·∫øu ch∆∞a c√≥ t·ª´ Wikipedia API)
            if relevance_score is None:
                relevance_score = calculate_relevance_score({
                    'title': title,
                    'snippet': snippet,
                    'url': url
                }, query)
            
            # N·∫øu h·ªèi v·ªÅ "h√¥m nay" ho·∫∑c ng√†y c·ª• th·ªÉ -> ∆Øu ti√™n k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i
            date_bonus = 0.0
            has_current_date_in_result = False
            if has_today or prioritize_today:
                title_lower = title.lower()
                snippet_lower = snippet.lower()
                
                # Ki·ªÉm tra xem c√≥ ng√†y hi·ªán t·∫°i trong title/snippet kh√¥ng (ch·ªâ ki·ªÉm tra 1 l·∫ßn)
                has_current_date_in_result = (
                    current_date_str in title or current_date_str in snippet or
                    current_date_str_short in title or current_date_str_short in snippet or
                    current_date_str_dash in title or current_date_str_dash in snippet or
                    'h√¥m nay' in title_lower or 'h√¥m nay' in snippet_lower
                )
                
                # Bonus ƒëi·ªÉm n·∫øu c√≥ ng√†y hi·ªán t·∫°i (∆∞u ti√™n cao)
                if has_current_date_in_result:
                    date_bonus = 0.5  # Bonus l·ªõn ƒë·ªÉ ∆∞u ti√™n k·∫øt qu·∫£ m·ªõi nh·∫•t
                    logging.info(f"  üìÖ [{i}] C√ì NG√ÄY HI·ªÜN T·∫†I - Bonus +0.5 | {title[:60]}...")
                else:
                    # N·∫øu prioritize_today=True nh∆∞ng kh√¥ng c√≥ ng√†y hi·ªán t·∫°i -> gi·∫£m ƒëi·ªÉm
                    if prioritize_today:
                        # V·∫´n l·∫•y nh∆∞ng gi·∫£m ƒëi·ªÉm m·ªôt ch√∫t
                        date_bonus = -0.1
                        logging.info(f"  ‚ö†Ô∏è [{i}] KH√îNG C√ì NG√ÄY HI·ªÜN T·∫†I (nh∆∞ng v·∫´n l·∫•y) | {title[:60]}...")
            
            # √Åp d·ª•ng bonus ƒëi·ªÉm
            relevance_score += date_bonus
            relevance_score = min(relevance_score, 1.0)  # Gi·ªõi h·∫°n t·ªëi ƒëa 1.0
            
            # Ch·ªâ l·∫•y k·∫øt qu·∫£ c√≥ ƒëi·ªÉm li√™n quan >= 0.2 (tr√°nh k·∫øt qu·∫£ ho√†n to√†n kh√¥ng li√™n quan)
            if relevance_score < 0.2:
                logging.info(f"  ‚úó [{i}] ƒêI·ªÇM TH·∫§P ({relevance_score:.2f}) | {title[:60]}...")
                continue
            
            result_dict = {
                'title': title,
                'snippet': snippet,
                'url': url,
                'relevance_score': relevance_score,
                'is_wikipedia': is_wikipedia,
                'has_current_date': has_current_date_in_result  # L∆∞u ƒë·ªÉ s·∫Øp x·∫øp
            }
            
            scored_results.append(result_dict)
            logging.info(f"  ‚úÖ [{i}] ƒêi·ªÉm: {relevance_score:.2f} | {title[:60]}...")
        
        if not scored_results:
            logging.error(f"  ‚ùå KH√îNG t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p!")
            return {'error': 'Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p'}
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm li√™n quan (cao nh·∫•t tr∆∞·ªõc)
        # ∆Øu ti√™n: 1) K·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i (n·∫øu prioritize_today), 2) Wikipedia, 3) ƒêi·ªÉm li√™n quan
        if prioritize_today:
            scored_results.sort(key=lambda x: (
                x.get('has_current_date', False),  # ∆Øu ti√™n cao nh·∫•t: c√≥ ng√†y hi·ªán t·∫°i
                x['is_wikipedia'],  # Sau ƒë√≥ Wikipedia
                x['relevance_score']  # Cu·ªëi c√πng l√† ƒëi·ªÉm li√™n quan
            ), reverse=True)
        else:
            # S·∫Øp x·∫øp b√¨nh th∆∞·ªùng: Wikipedia tr∆∞·ªõc, sau ƒë√≥ ƒëi·ªÉm li√™n quan
            scored_results.sort(key=lambda x: (x['is_wikipedia'], x['relevance_score']), reverse=True)
        
        # ===== CHATGPT STYLE: ∆ØU TI√äN DOMAIN CHUY√äN BI·ªÜT =====
        
        selected_sources = []
        query_lower = query.lower()
        
        # 1. ∆ØU TI√äN CAO NH·∫§T: Wikipedia API results (ƒë√£ c√≥ relevance_score = 1.0)
        wikipedia_api_results = [r for r in scored_results if r.get('is_wikipedia') and r.get('relevance_score', 0) >= 0.9]
        if wikipedia_api_results:
            # Th√™m t·∫•t c·∫£ Wikipedia API results (th∆∞·ªùng ch·ªâ c√≥ 1-3)
            for wiki_result in wikipedia_api_results[:2]:  # T·ªëi ƒëa 2 t·ª´ Wikipedia API
                selected_sources.append(wiki_result)
                logging.info(f"  ‚≠ê‚≠ê WIKIPEDIA API (∆∞u ti√™n cao nh·∫•t): {wiki_result['title'][:60]}")
        
        # 1.5. ∆ØU TI√äN CAO: Bing Search API results (ƒë√£ c√≥ relevance_score = 0.9)
        bing_api_results = [r for r in scored_results if isinstance(r, dict) and r.get('source') == 'bing_api']
        if bing_api_results and len(selected_sources) < 3:
            # Th√™m Bing results (k·∫øt qu·∫£ r·∫•t t·ªët)
            for bing_result in bing_api_results[:2]:  # T·ªëi ƒëa 2 t·ª´ Bing API
                if bing_result not in selected_sources:
                    selected_sources.append(bing_result)
                    logging.info(f"  ‚≠ê‚≠ê BING API (∆∞u ti√™n cao): {bing_result['title'][:60]}")
        
        # 2. ∆Øu ti√™n domain chuy√™n bi·ªát theo ch·ªß ƒë·ªÅ (n·∫øu ch∆∞a ƒë·ªß)
        priority_result = None
        
        # Gi√° v√†ng ‚Üí ∆Øu ti√™n k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i (n·∫øu prioritize_today=True)
        if 'gi√° v√†ng' in query_lower or 'v√†ng sjc' in query_lower:
            # Danh s√°ch domain uy t√≠n v·ªÅ gi√° v√†ng (∆∞u ti√™n cao)
            gold_trusted_domains = [
                'thanhnien.vn', 'cafef.vn', 'vnexpress.net', 'dantri.com.vn',
                'tuoitre.vn', 'vneconomy.vn', 'ndh.vn', 'bnews.vn',
                '24h.com.vn', 'giavang.net', 'giavang.org.vn'
            ]
            
            # N·∫øu prioritize_today=True, ∆∞u ti√™n k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i tr∆∞·ªõc
            if prioritize_today:
                # B∆∞·ªõc 1: T√¨m k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i V√Ä domain uy t√≠n
                for result in scored_results:
                    title_lower = result['title'].lower()
                    snippet_lower = result.get('snippet', '').lower()
                    has_current_date = (
                        current_date_str in result['title'] or current_date_str in result.get('snippet', '') or
                        current_date_str_short in result['title'] or current_date_str_short in result.get('snippet', '') or
                        'h√¥m nay' in title_lower or 'h√¥m nay' in snippet_lower
                    )
                    is_trusted = any(domain in result['url'] for domain in gold_trusted_domains)
                    is_irrelevant = any(kw in title_lower or kw in snippet_lower for kw in irrelevant_keywords)
                    
                    if has_current_date and not is_irrelevant and result not in selected_sources:
                        if is_trusted:
                            priority_result = result
                            logging.info(f"  ‚≠ê‚≠ê‚≠ê ∆ØU TI√äN CAO NH·∫§T (c√≥ ng√†y + domain uy t√≠n): {result['title'][:60]}")
                            break
                        elif not priority_result:  # L∆∞u t·∫°m n·∫øu ch∆∞a c√≥ k·∫øt qu·∫£ t·ªët h∆°n
                            priority_result = result
                            logging.info(f"  ‚≠ê‚≠ê ∆ØU TI√äN (c√≥ ng√†y hi·ªán t·∫°i): {result['title'][:60]}")
                
                # B∆∞·ªõc 2: N·∫øu ch∆∞a c√≥ k·∫øt qu·∫£ c√≥ ng√†y hi·ªán t·∫°i, t√¨m domain uy t√≠n
                if not priority_result:
                    for result in scored_results:
                        is_irrelevant = any(kw in result['title'].lower() or kw in result.get('snippet', '').lower() 
                                          for kw in irrelevant_keywords)
                        if any(domain in result['url'] for domain in gold_trusted_domains) and not is_irrelevant and result not in selected_sources:
                            priority_result = result
                            logging.info(f"  ‚≠ê ∆ØU TI√äN DOMAIN UY T√çN (gi√° v√†ng): {result['title'][:60]}")
                            break
            else:
                # N·∫øu kh√¥ng prioritize_today, ∆∞u ti√™n domain uy t√≠n
                for result in scored_results:
                    is_irrelevant = any(kw in result['title'].lower() or kw in result.get('snippet', '').lower() 
                                      for kw in irrelevant_keywords)
                    if any(domain in result['url'] for domain in gold_trusted_domains) and not is_irrelevant and result not in selected_sources:
                        priority_result = result
                        logging.info(f"  ‚≠ê ∆ØU TI√äN DOMAIN UY T√çN (gi√° v√†ng): {result['title'][:60]}")
                        break
        
        # Th·ªùi ti·∫øt ‚Üí accuweather.com ho·∫∑c nchmf.gov.vn
        elif 'th·ªùi ti·∫øt' in query_lower or 'nhi·ªát ƒë·ªô' in query_lower or 'd·ª± b√°o' in query_lower:
            for result in scored_results:
                if ('accuweather.com' in result['url'] or 'nchmf.gov.vn' in result['url']) and result not in selected_sources:
                    priority_result = result
                    logging.info(f"  ‚≠ê ∆ØU TI√äN ACCUWEATHER (th·ªùi ti·∫øt): {result['title'][:60]}")
                    break
        
        # Th√™m domain ∆∞u ti√™n n·∫øu c√≥
        if priority_result and len(selected_sources) < 3:
            selected_sources.append(priority_result)
        
        # 3. L·∫•y c√°c ngu·ªìn Wikipedia kh√°c (t·ª´ DuckDuckGo, kh√¥ng ph·∫£i API)
        wikipedia_other_results = [r for r in scored_results if r['is_wikipedia'] and r not in selected_sources]
        if wikipedia_other_results and len(selected_sources) < 3:
            selected_sources.append(wikipedia_other_results[0])
            logging.info(f"  ‚≠ê WIKIPEDIA (t·ª´ DuckDuckGo): {wikipedia_other_results[0]['title'][:60]}")
        
        # 4. Th√™m c√°c ngu·ªìn kh√°c t·ªët nh·∫•t (t·ªëi ƒëa 3 ngu·ªìn t·ªïng c·ªông)
        other_results = [r for r in scored_results if r not in selected_sources]
        max_sources = 3
        for result in other_results:
            if len(selected_sources) >= max_sources:
                break
            selected_sources.append(result)
            logging.info(f"  ‚úÖ Th√™m ngu·ªìn: {result['title'][:60]} (ƒëi·ªÉm: {result['relevance_score']:.2f})")
        
        logging.info(f"  üìä ƒê√£ ch·ªçn {len(selected_sources)} ngu·ªìn ƒë·ªÉ ph√¢n t√≠ch")
        
        if blocked_count > 0:
            logging.info(f"üö´ ƒê√£ ch·∫∑n {blocked_count} ngu·ªìn (Mojeek)")
        
        return {'results': selected_sources}
        
    except Exception as e:
        logging.error(f"‚ùå L·ªói web search: {e}")
        import traceback
        logging.error(traceback.format_exc())
        return {'error': f'L·ªói t√¨m ki·∫øm: {str(e)}'}


def calculate(expression: str) -> dict:
    """
    T√≠nh to√°n bi·ªÉu th·ª©c to√°n h·ªçc
    
    Args:
        expression: Bi·ªÉu th·ª©c to√°n h·ªçc (v√≠ d·ª•: "2+2", "sqrt(16)", "sin(pi/2)")
    
    Returns:
        Dict v·ªõi result ho·∫∑c error
    """
    try:
        logging.info(f"üî¢ Calculate: '{expression}'")
        
        # Parse v√† t√≠nh to√°n v·ªõi sympy
        result = sympy.sympify(expression)
        result_value = float(result.evalf())
        
        logging.info(f"‚úÖ K·∫øt qu·∫£: {result_value}")
        return {
            'expression': expression,
            'result': result_value,
            'formatted': f"{expression} = {result_value}"
        }
        
    except Exception as e:
        logging.error(f"‚ùå L·ªói calculate: {e}")
        return {'error': f'L·ªói t√≠nh to√°n: {str(e)}'}


def execute_code(code: str, timeout_seconds: int = 5) -> dict:
    """
    Th·ª±c thi code Python trong sandbox an to√†n
    
    Args:
        code: Code Python c·∫ßn ch·∫°y
        timeout_seconds: Timeout (gi√¢y)
    
    Returns:
        Dict v·ªõi output ho·∫∑c error
    """
    try:
        logging.info(f"üíª Execute Code:\n{code[:100]}...")
        
        # Compile code v·ªõi RestrictedPython
        byte_code = compile_restricted_exec(code)
        
        if byte_code.errors:
            return {'error': f'L·ªói c√∫ ph√°p: {byte_code.errors}'}
        
        # T·∫°o safe environment
        safe_env = {
            '__builtins__': safe_globals,
            '_print_': lambda x: print(x),
            '_getattr_': getattr,
        }
        
        # Th√™m c√°c module an to√†n
        import math
        safe_env['math'] = math
        
        # Capture output
        from io import StringIO
        import sys
        old_stdout = sys.stdout
        sys.stdout = output_buffer = StringIO()
        
        try:
            # Execute v·ªõi timeout
            exec(byte_code.code, safe_env)
            output = output_buffer.getvalue()
            
            logging.info(f"‚úÖ Code executed successfully")
            return {
                'output': output if output else 'Code ch·∫°y th√†nh c√¥ng (kh√¥ng c√≥ output)',
                'success': True
            }
        finally:
            sys.stdout = old_stdout
        
    except TimeoutError:
        logging.error("‚ùå Code execution timeout")
        return {'error': 'Timeout: Code ch·∫°y qu√° 5 gi√¢y'}
    except Exception as e:
        logging.error(f"‚ùå L·ªói execute code: {e}")
        return {'error': f'L·ªói runtime: {str(e)}'}


def detect_tool_needed(question: str) -> str:
    """
    Ph√°t hi·ªán tool n√†o c·∫ßn d√πng d·ª±a v√†o c√¢u h·ªèi - TH√îNG MINH nh∆∞ ChatGPT
    
    Returns:
        'search' | 'calculate' | 'code' | 'clarify' | 'chat'
    """
    question_lower = question.lower()
    
    # ===== 0. PRICE/COMMODITY SEARCH - ∆Øu ti√™n cao nh·∫•t =====
    # Tr√°nh nh·∫ßm "gi√° v√†ng" v·ªõi "chia"
    price_keywords = ['gi√° v√†ng', 'gi√° d·∫ßu', 'gi√° xƒÉng', 'gi√° bitcoin', 'gi√° usd', 'gi√° vnd', 't·ª∑ gi√°', 'gi√° c·ªï phi·∫øu', 'gi√° nh√†', 'gi√° ƒë·∫•t']
    if any(kw in question_lower for kw in price_keywords):
        return 'search'
    
    # ===== 1. CALCULATOR - Ch·ªâ khi c√≥ s·ªë c·ª• th·ªÉ V√Ä c√≥ ph√©p t√≠nh =====
    calc_keywords = [
        'calculate', 'b·∫±ng bao nhi√™u', 'b·∫±ng', 'c·ªông', 'tr·ª´', 'nh√¢n',
        '+', '-', '*', '/', 'sqrt', 'sin', 'cos', 'tan', '^', '**', '='
    ]
    
    # Ki·ªÉm tra c√≥ ph√©p to√°n r√µ r√†ng
    has_math_operator = re.search(r'\d+\s*[\+\-\*/\^]\s*\d+', question)  # VD: 3+5, 10*2
    has_calc_keyword = any(kw in question_lower for kw in calc_keywords)
    
    if has_math_operator or (has_calc_keyword and re.search(r'\d+', question)):
        # Ch·ªâ calculator khi c√≥ ph√©p t√≠nh ho·∫∑c keyword + s·ªë
        return 'calculate'
    
    # ===== 2. CODE EXECUTION =====
    code_keywords = [
        'ch·∫°y code', 'execute', 'run python', 'vi·∫øt code',
        'def ', 'for ', 'while ', 'print(', 'import ',
        'fibonacci', 'prime', 'sort', 'algorithm'
    ]
    if any(kw in question_lower for kw in code_keywords):
        return 'code'
    
    # ===== 3. WEB SEARCH - T·ª∞ ƒê·ªòNG NH·∫¨N BI·∫æT =====
    
    # 3.1. Keywords r√µ r√†ng
    explicit_search_keywords = [
        't√¨m ki·∫øm', 'search', 'tra c·ª©u', 't√¨m', 'google',
        'tin t·ª©c', 'th√¥ng tin m·ªõi', 'tin m·ªõi', 'c·∫≠p nh·∫≠t'
    ]
    if any(kw in question_lower for kw in explicit_search_keywords):
        return 'search'
    
    # 3.2. Th·ªùi gian realtime (h√¥m nay, hi·ªán t·∫°i, nƒÉm 2025...)
    time_keywords = ['h√¥m nay', 'hi·ªán t·∫°i', 'b√¢y gi·ªù', 'm·ªõi nh·∫•t', 'nƒÉm 2025', 'th√°ng 11']
    if any(kw in question_lower for kw in time_keywords):
        return 'search'
    
    # 3.3. C√¢u h·ªèi v·ªÅ ng∆∞·ªùi/ƒë·ªãa ƒëi·ªÉm/s·ª± ki·ªán c·ª• th·ªÉ
    wh_questions = ['ai l√†', 'ai ƒë√£', 'khi n√†o', '·ªü ƒë√¢u', 't·∫°i sao', 'nh∆∞ th·∫ø n√†o', 'c√≥ ph·∫£i']
    if any(kw in question_lower for kw in wh_questions):
        # Ki·ªÉm tra c√≥ t√™n ri√™ng (ch·ªØ hoa) ho·∫∑c t√™n ng∆∞·ªùi/ƒë·ªãa ƒëi·ªÉm
        if re.search(r'[A-Z][a-z]+|vi·ªát nam|h√† n·ªôi|s√†i g√≤n|m·ªπ|trung qu·ªëc', question):
            return 'search'
    
    # 3.4. Lƒ©nh v·ª±c c·∫ßn th√¥ng tin m·ªõi (gi√° c·∫£, th·ªùi ti·∫øt, th·ªÉ thao, ch√≠nh tr·ªã...)
    realtime_topics = [
        'gi√°', 'th·ªùi ti·∫øt', 'nhi·ªát ƒë·ªô', 'd·ª± b√°o', 't·ª∑ gi√°', 'ch·ª©ng kho√°n',
        'b√≥ng ƒë√°', 'world cup', 'olympic', 'gi·∫£i ƒë·∫•u', 'k·∫øt qu·∫£', 't·ª∑ s·ªë',
        't·ªïng th·ªëng', 'th·ªß t∆∞·ªõng', 'ch√≠nh ph·ªß', 'qu·ªëc h·ªôi', 'b·∫ßu c·ª≠',
        'covid', 'd·ªãch b·ªánh', 'vaccine', 'ca nhi·ªÖm',
        'chi·∫øn tranh', 'xung ƒë·ªôt', 'bi·ªÉn ƒë√¥ng',
        'c√¥ng ngh·ªá m·ªõi', 'ƒëi·ªán tho·∫°i', 'iphone', 'samsung', 'tesla', 'ai', 'chatgpt'
    ]
    if any(topic in question_lower for topic in realtime_topics):
        return 'search'
    
    # 3.5. S·ªë li·ªáu th·ªëng k√™ (bao nhi√™u ng∆∞·ªùi, bao nhi√™u t·ªânh, d√¢n s·ªë...)
    stat_keywords = ['bao nhi√™u', 's·ªë l∆∞·ª£ng', 'th·ªëng k√™', 'd√¢n s·ªë', 'di·ªán t√≠ch', 'chi·ªÅu cao']
    if any(kw in question_lower for kw in stat_keywords):
        # Tr·ª´ c√¢u h·ªèi to√°n h·ªçc ƒë∆°n gi·∫£n
        if not re.search(r'\d+\s*[+\-*/]\s*\d+', question):
            return 'search'
    
    # ===== 4. CLARIFY - C√¢u h·ªèi m∆° h·ªì =====
    clarify_keywords = ['c√°i ƒë√≥', 'n√≥', 'g√¨ ƒë√≥', 'th·ª© g√¨', 'm·∫•y c√°i', 'm·ªôt s·ªë', 'v√†i']
    if any(kw in question_lower for kw in clarify_keywords):
        # N·∫øu c√¢u h·ªèi qu√° ng·∫Øn v√† m∆° h·ªì
        if len(question.split()) < 5:
            return 'clarify'
    
    # ===== 5. CHAT - M·∫∑c ƒë·ªãnh =====
    return 'chat'

def query_lm_studio(messages: list, stream: bool = False) -> dict:
    """
    G·ª≠i request ƒë·∫øn LM Studio v·ªõi GPU acceleration
    
    Args:
        messages: List of message dicts [{'role': 'user', 'content': '...'}]
        stream: True ƒë·ªÉ streaming response, False ƒë·ªÉ response ƒë·∫ßy ƒë·ªß
    
    Returns:
        Response dict t·ª´ LM Studio (ho·∫∑c generator n·∫øu stream=True)
    """
    payload = {
        'model': LM_STUDIO_MODEL,
        'messages': messages,
        'temperature': 0.8,
        'max_tokens': 3096,
        'top_p': 0.95,
        'frequency_penalty': 0.3,
        'presence_penalty': 0.3,
        'stream': stream,  # H·ªó tr·ª£ streaming
        # GPU optimization parameters
        'num_gpu': 1,
        'gpu_layers': 35,
    }
    
    try:
        logging.info(f"G·ª≠i request ƒë·∫øn LM Studio (stream={stream})")
        
        response = requests.post(
            LM_STUDIO_URL,
            json=payload,
            timeout=60,
            stream=stream  # Streaming mode
        )
        
        if response.status_code != 200:
            logging.error(f"LM Studio tr·∫£ v·ªÅ l·ªói: {response.status_code}")
            return None
        
        # N·∫øu streaming, tr·∫£ v·ªÅ response object ƒë·ªÉ iterate
        if stream:
            return response
        
        # N·∫øu kh√¥ng stream, parse JSON
        result = response.json()
        logging.info("Nh·∫≠n ƒë∆∞·ª£c response t·ª´ LM Studio")
        return result
        
    except requests.exceptions.ConnectionError:
        logging.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn LM Studio t·∫°i {LM_STUDIO_URL}")
        logging.error("H√£y ch·∫Øc ch·∫Øn LM Studio ƒëang ch·∫°y v√† model ƒë√£ load!")
        return None
    except Exception as e:
        logging.error(f"L·ªói khi g·ªçi LM Studio: {e}")
        return None


# ===== FLASK ROUTES =====

@app.route('/')
def index():
    """Serve frontend"""
    return render_template('index.html')


@app.route('/static/<path:filename>')
def serve_static(filename):
    """Serve static files"""
    return send_from_directory('static', filename)


@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'ok'
    })


@app.route('/query', methods=['POST'])
def query():
    """Endpoint ch√≠nh: G·ª≠i c√¢u h·ªèi ƒë·∫øn LM Studio"""
    data = request.get_json()
    if not data or 'question' not in data:
        return jsonify({'error': 'Thi·∫øu tham s·ªë question'}), 400
    
    question = data['question']
    session_id = data.get('session_id', 'default')
    
    # Ki·ªÉm tra n·∫øu l√† l·ªùi ch√†o
    if is_greeting(question):
        answer = "Ch√†o B·∫°n! T√¥i l√† Chatbot AI 37 c√≥ th·ªÉ tr·∫£ l·ªùi t·∫•t c·∫£ c√¢u h·ªèi c·ªßa b·∫°n."
        add_to_memory(session_id, 'user', question)
        add_to_memory(session_id, 'assistant', answer)
        return jsonify({'reply': answer})
    
    # L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i
    history = get_conversation_history(session_id)
    
    # T·∫°o messages
    msgs = [{'role': 'system', 'content': SYSTEM_PROMPT}]
    msgs.extend(history)
    msgs.append({'role': 'user', 'content': question})
    
    # G·ªçi LM Studio
    lm_resp = query_lm_studio(msgs)
    
    if not lm_resp:
        return jsonify({'error': 'L·ªói k·∫øt n·ªëi LM Studio'}), 500
    
    try:
        answer = lm_resp['choices'][0]['message']['content']
        answer = clean_response(answer)
        
        # L∆∞u v√†o memory
        add_to_memory(session_id, 'user', question)
        add_to_memory(session_id, 'assistant', answer)
        
        return jsonify({'reply': answer})
    except Exception as e:
        logging.error(f"L·ªói x·ª≠ l√Ω response: {e}")
        return jsonify({'error': 'L·ªói x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi'}), 500



@app.route('/chat', methods=['GET', 'POST'])
def chat():
    """Endpoint chat cho frontend - H·ªñ TR·ª¢ STREAMING"""
    
    # X·ª≠ l√Ω GET request (th∆∞·ªùng do browser cache)
    if request.method == 'GET':
        return jsonify({
            'message': 'Endpoint /chat ch·ªâ ch·∫•p nh·∫≠n POST request',
            'usage': 'POST /chat v·ªõi body: {"message": "c√¢u h·ªèi", "stream": true}'
        }), 200
    
    data = request.get_json()
    
    if not data or 'message' not in data:
        return jsonify({'error': 'Thi·∫øu tham s·ªë message'}), 400
    
    question = data['message']
    session_id = data.get('session_id', 'default')
    use_stream = data.get('stream', True)  # M·∫∑c ƒë·ªãnh b·∫≠t streaming
    
    logging.info(f"üì® Nh·∫≠n c√¢u h·ªèi: '{question}' (stream={use_stream})")
    
    # Ki·ªÉm tra n·∫øu l√† l·ªùi ch√†o ƒë∆°n gi·∫£n
    if is_greeting(question):
        answer = "Ch√†o b·∫°n! T√¥i l√† AI 37, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
        add_to_memory(session_id, 'user', question)
        add_to_memory(session_id, 'assistant', answer)
        
        # N·∫øu streaming, tr·∫£ v·ªÅ streaming format
        if use_stream:
            def greeting_stream():
                # G·ª≠i t·ª´ng ch·ªØ ƒë·ªÉ gi·ªëng streaming
                for char in answer:
                    yield f"data: {json.dumps({'content': char})}\n\n"
                yield f"data: {json.dumps({'done': True})}\n\n"
            
            return Response(
                stream_with_context(greeting_stream()),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
        else:
            return jsonify({'reply': answer, 'response': answer})
    
    # L·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i
    history = get_conversation_history(session_id)
    
    # Ki·ªÉm tra follow-up question - n·∫øu l√† follow-up, kh√¥ng c·∫ßn search l·∫°i
    is_follow_up = is_follow_up_question(question, history)
    
    if is_follow_up:
        logging.info(f"üí¨ Detected follow-up question - using context from previous conversation")
        # Kh√¥ng g·ªçi tool, ƒë·ªÉ LLM t·ª± tr·∫£ l·ªùi d·ª±a v√†o context trong history
        tool_needed = 'chat'
        
        # Th√™m context t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë·ªÉ LLM hi·ªÉu r√µ h∆°n
        last_assistant_msg = None
        for msg in reversed(history):
            if msg.get('role') == 'assistant':
                last_assistant_msg = msg.get('content', '')
                break
        
        if last_assistant_msg and len(last_assistant_msg) > 50:
            # Th√™m context ng·∫Øn g·ªçn t·ª´ c√¢u tr·∫£ l·ªùi tr∆∞·ªõc
            tool_context = f"\n\nüí¨ NG·ªÆ C·∫¢NH T·ª™ C√ÇU TR·∫¢ L·ªúI TR∆Ø·ªöC:\n{last_assistant_msg[:500]}\n\n"
            tool_context += "‚ö†Ô∏è Y√äU C·∫¶U: Tr·∫£ l·ªùi d·ª±a v√†o ng·ªØ c·∫£nh tr√™n, m·ªü r·ªông th√¥ng tin n·∫øu c·∫ßn. Tr·∫£ l·ªùi T·ª∞ NHI√äN, nh∆∞ th·ªÉ b·∫°n ƒëang ti·∫øp t·ª•c c√¢u chuy·ªán.\n\n"
    else:
        # GIAI ƒêO·∫†N 2: Detect tool needed
        tool_needed = detect_tool_needed(question)
        logging.info(f"üîß Tool detected: {tool_needed}")
    
    tool_result = None
    tool_context = ""
    direct_answer = None
    
    # G·ªçi tool n·∫øu c·∫ßn
    if tool_needed == 'search':
        question_lower = question.lower()
        
        # ===== CASE 1: TH·ªúI TI·∫æT ‚Üí G·ªåI ACCUWEATHER API =====
        if 'th·ªùi ti·∫øt' in question_lower or 'nhi·ªát ƒë·ªô' in question_lower or 'd·ª± b√°o' in question_lower:
            logging.info("üå§Ô∏è Detected weather query - calling AccuWeather API")
            
            # Tr√≠ch xu·∫•t t√™n th√†nh ph·ªë t·ª´ c√¢u h·ªèi (danh s√°ch ƒë·∫ßy ƒë·ªß 63 t·ªânh th√†nh)
            location = "Hanoi"  # M·∫∑c ƒë·ªãnh
            
            # C√°c th√†nh ph·ªë l·ªõn (d√πng location_key tr·ª±c ti·∫øp ƒë·ªÉ ti·∫øt ki·ªám API calls)
            location_key = None
            city_name = None
            
            if 'h√† n·ªôi' in question_lower or 'hanoi' in question_lower:
                location_key = "353412"  # Hanoi
                city_name = "H√† N·ªôi"
            elif 's√†i g√≤n' in question_lower or 'h·ªì ch√≠ minh' in question_lower or 'saigon' in question_lower or 'tp hcm' in question_lower:
                location_key = "353981"  # Ho Chi Minh City
                city_name = "TP H·ªì Ch√≠ Minh"
            elif 'ƒë√† n·∫µng' in question_lower or 'da nang' in question_lower:
                location_key = "353926"  # Da Nang
                city_name = "ƒê√† N·∫µng"
            elif 'h·∫£i ph√≤ng' in question_lower or 'hai phong' in question_lower:
                location_key = "353346"  # Hai Phong
                city_name = "H·∫£i Ph√≤ng"
            elif 'c·∫ßn th∆°' in question_lower or 'can tho' in question_lower:
                location_key = "353933"  # Can Tho
                city_name = "C·∫ßn Th∆°"
            
            # C√°c t·ªânh mi·ªÅn B·∫Øc
            elif 'ngh·ªá an' in question_lower or 'nghe an' in question_lower or 'vinh' in question_lower:
                location = "Vinh"
            elif 'thanh h√≥a' in question_lower or 'thanh hoa' in question_lower:
                location = "Thanh Hoa"
            elif 'h√† tƒ©nh' in question_lower or 'ha tinh' in question_lower:
                location = "Ha Tinh"
            elif 'qu·∫£ng ninh' in question_lower or 'quang ninh' in question_lower or 'h·∫° long' in question_lower:
                location = "Ha Long"
            elif 'l√†o cai' in question_lower or 'lao cai' in question_lower or 'sapa' in question_lower:
                location = "Lao Cai"
            
            # C√°c t·ªânh mi·ªÅn Trung
            elif 'hu·∫ø' in question_lower or 'hue' in question_lower or 'th·ª´a thi√™n' in question_lower:
                location = "Hue"
            elif 'qu·∫£ng nam' in question_lower or 'quang nam' in question_lower or 'h·ªôi an' in question_lower:
                location = "Tam Ky"
            elif 'qu·∫£ng ng√£i' in question_lower or 'quang ngai' in question_lower:
                location = "Quang Ngai"
            elif 'b√¨nh ƒë·ªãnh' in question_lower or 'binh dinh' in question_lower or 'quy nh∆°n' in question_lower:
                location = "Quy Nhon"
            elif 'ph√∫ y√™n' in question_lower or 'phu yen' in question_lower or 'tuy h√≤a' in question_lower:
                location = "Tuy Hoa"
            elif 'kh√°nh h√≤a' in question_lower or 'khanh hoa' in question_lower or 'nha trang' in question_lower:
                location = "Nha Trang"
            
            # T√¢y Nguy√™n
            elif 'ƒë·∫Øk l·∫Øk' in question_lower or 'dak lak' in question_lower or 'bu√¥n ma thu·ªôt' in question_lower:
                location = "Buon Ma Thuot"
            elif 'l√¢m ƒë·ªìng' in question_lower or 'lam dong' in question_lower or 'ƒë√† l·∫°t' in question_lower or 'da lat' in question_lower:
                location = "Da Lat"
            elif 'gia lai' in question_lower or 'pleiku' in question_lower:
                location = "Pleiku"
            
            # Mi·ªÅn Nam
            elif 'b√¨nh d∆∞∆°ng' in question_lower or 'binh duong' in question_lower or 'th·ªß d·∫ßu m·ªôt' in question_lower:
                location = "Thu Dau Mot"
            elif 'ƒë·ªìng nai' in question_lower or 'dong nai' in question_lower or 'bi√™n h√≤a' in question_lower:
                location = "Bien Hoa"
            elif 'b√† r·ªãa' in question_lower or 'ba ria' in question_lower or 'v≈©ng t√†u' in question_lower or 'vung tau' in question_lower:
                location = "Vung Tau"
            elif 'long an' in question_lower or 't√¢n an' in question_lower:
                location = "Tan An"
            elif 'ti·ªÅn giang' in question_lower or 'tien giang' in question_lower or 'm·ªπ tho' in question_lower:
                location = "My Tho"
            elif 'vƒ©nh long' in question_lower or 'vinh long' in question_lower:
                location = "Vinh Long"
            elif 'an giang' in question_lower or 'long xuy√™n' in question_lower:
                location = "Long Xuyen"
            elif 'ki√™n giang' in question_lower or 'kien giang' in question_lower or 'r·∫°ch gi√°' in question_lower:
                location = "Rach Gia"
            elif 'c√† mau' in question_lower or 'ca mau' in question_lower:
                location = "Ca Mau"
            
            # ===== QUY TR√åNH CHATGPT: WEB SEARCH + TR√çCH XU·∫§T TH√îNG MINH =====
            if not city_name:
                city_name = location  # Fallback cho c√°c t·ªânh kh√¥ng c√≥ location_key
            
            weather_result = get_weather_chatgpt_style(city_name, question)
            
            if 'error' not in weather_result:
                # Ki·ªÉm tra xem c√≥ c√¢u h·ªèi v·ªÅ gi·ªù c·ª• th·ªÉ kh√¥ng
                target_hour = extract_hour_from_question(question)
                
                # T·∫°o context t·ª´ d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t v√† chu·∫©n h√≥a
                tool_context = f"\n\nüå§Ô∏è TH·ªúI TI·∫æT {weather_result['city'].upper()} H√îM NAY:\n\n"
                
                # N·∫øu c√≥ c√¢u h·ªèi v·ªÅ gi·ªù c·ª• th·ªÉ
                if target_hour is not None and 'hourly_temperature' in weather_result:
                    hourly_temp = weather_result['hourly_temperature']
                    is_estimated = weather_result.get('estimated', False)
                    
                    tool_context += f"‚è∞ Th·ªùi ti·∫øt l√∫c {target_hour}h: {hourly_temp}¬∞C"
                    if is_estimated:
                        tool_context += " (∆∞·ªõc t√≠nh)"
                    tool_context += "\n"
                    
                    if weather_result['conditions']:
                        tool_context += f"‚òÅÔ∏è ƒêi·ªÅu ki·ªán: {weather_result['conditions']}\n"
                    
                    if weather_result['humidity']:
                        tool_context += f"üíß ƒê·ªô ·∫©m: {weather_result['humidity']}%\n"
                    
                    tool_context += "\n"
                    tool_context += "‚ö†Ô∏è Y√äU C·∫¶U: Tr·∫£ l·ªùi 1-2 c√¢u t·ª± nhi√™n v·ªÅ th·ªùi ti·∫øt l√∫c gi·ªù ƒë√≥.\n"
                    conditions_text = weather_result.get('conditions', 'tr·ªùi')
                    tool_context += f"V√≠ d·ª•: 'H√† N·ªôi l√∫c {target_hour}h kho·∫£ng {hourly_temp}¬∞C, {conditions_text}.'\n\n"
                
                # C√¢u h·ªèi chung v·ªÅ th·ªùi ti·∫øt
                else:
                    if weather_result.get('current_temperature'):
                        tool_context += f"üå°Ô∏è Nhi·ªát ƒë·ªô hi·ªán t·∫°i: {weather_result['current_temperature']}¬∞C\n"
                    
                    if weather_result.get('temperature_min') and weather_result.get('temperature_max'):
                        tool_context += f"üå°Ô∏è Nhi·ªát ƒë·ªô: {weather_result['temperature_min']}¬∞C - {weather_result['temperature_max']}¬∞C\n"
                    elif weather_result.get('temperature_avg'):
                        tool_context += f"üå°Ô∏è Nhi·ªát ƒë·ªô: kho·∫£ng {weather_result['temperature_avg']}¬∞C\n"
                    
                    if weather_result.get('humidity'):
                        tool_context += f"üíß ƒê·ªô ·∫©m: {weather_result['humidity']}%\n"
                    
                    if weather_result.get('conditions'):
                        tool_context += f"‚òÅÔ∏è T√¨nh tr·∫°ng: {weather_result['conditions']}\n"
                    
                    tool_context += f"\nüìö Ngu·ªìn: AccuWeather\n\n"
                    tool_context += "‚ö†Ô∏è Y√äU C·∫¶U: Tr·∫£ l·ªùi T·ª∞ NHI√äN nh∆∞ ChatGPT, 2-3 c√¢u ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu.\n"
                    
                    # T·∫°o v√≠ d·ª• ƒë·ªông d·ª±a tr√™n d·ªØ li·ªáu th·ª±c
                    example_temp = weather_result.get('current_temperature') or weather_result.get('temperature_max') or '22'
                    example_condition = weather_result.get('conditions', 'tr·ªùi').split(',')[0] if weather_result.get('conditions') else 'tr·ªùi'
                    tool_context += f"V√≠ d·ª•: '{city_name} h√¥m nay kho·∫£ng {example_temp}¬∞C, {example_condition}, th·ªùi ti·∫øt d·ªÖ ch·ªãu.'\n\n"
                
                logging.info(f"‚úÖ [ChatGPT Style] Weather context created ({len(tool_context)} chars)")
            else:
                tool_context = f"\n\n[L·ªói l·∫•y th·ªùi ti·∫øt: {weather_result['error']}]"
        
        # ===== CASE 2: GI√Å V√ÄNG ‚Üí TR√çCH XU·∫§T GI√Å T·ª™ B√ÄI B√ÅO =====
        elif 'gi√° v√†ng' in question_lower or 'v√†ng sjc' in question_lower:
            logging.info("üí∞ Detected gold price query - extracting gold price")
            
            # Ph√°t hi·ªán n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ "h√¥m nay" ho·∫∑c ng√†y c·ª• th·ªÉ
            has_today = 'h√¥m nay' in question_lower
            today = datetime.now()
            current_date_str = today.strftime('%d/%m/%Y')
            current_date_str_short = today.strftime('%d/%m')
            
            # T·ªëi ∆∞u query t√¨m ki·∫øm: th√™m "h√¥m nay" ho·∫∑c ng√†y hi·ªán t·∫°i n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ h√¥m nay
            search_query = question
            if has_today:
                # Th√™m ng√†y hi·ªán t·∫°i v√†o query ƒë·ªÉ t√¨m k·∫øt qu·∫£ m·ªõi nh·∫•t
                search_query = f"{question} {current_date_str_short} {current_date_str}"
                logging.info(f"üìÖ User asked about today - adding date to query: {current_date_str}")
            
            # T√¨m ki·∫øm ƒë·ªÉ l·∫•y URL
            tool_result = search_web(search_query, max_results=10, prioritize_today=has_today)
            
            if 'results' in tool_result and tool_result['results']:
                # Th·ª≠ tr√≠ch xu·∫•t t·ª´ T·∫§T C·∫¢ c√°c ngu·ªìn (t·ªëi ƒëa 3 ngu·ªìn) ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
                sources = tool_result['results'][:3]
                gold_price = None
                best_source = None
                
                logging.info(f"üì∞ Th·ª≠ tr√≠ch xu·∫•t gi√° v√†ng t·ª´ {len(sources)} ngu·ªìn...")
                
                # Th·ª≠ t·ª´ng ngu·ªìn cho ƒë·∫øn khi t√¨m ƒë∆∞·ª£c gi√°
                for idx, source in enumerate(sources, 1):
                    url = source['url']
                    title = source['title']
                    
                    logging.info(f"   [{idx}] Th·ª≠ ngu·ªìn: {title[:60]}")
                    
                    # Fetch HTML content v·ªõi retry v√† error handling t·ªët h∆°n
                    try:
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'Connection': 'keep-alive',
                            'Upgrade-Insecure-Requests': '1'
                        }
                        
                        # Retry logic v·ªõi timeout tƒÉng d·∫ßn
                        max_retries = 2  # Gi·∫£m xu·ªëng 2 l·∫ßn ƒë·ªÉ nhanh h∆°n
                        response = None
                        for attempt in range(max_retries):
                            try:
                                response = requests.get(url, headers=headers, timeout=10, verify=True)
                                response.raise_for_status()
                                break
                            except requests.exceptions.SSLError as ssl_err:
                                if attempt < max_retries - 1:
                                    logging.warning(f"      ‚ö†Ô∏è SSL error, retrying with verify=False...")
                                    response = requests.get(url, headers=headers, timeout=10, verify=False)
                                    response.raise_for_status()
                                    break
                                else:
                                    raise
                            except requests.exceptions.RequestException as req_err:
                                if attempt < max_retries - 1:
                                    continue
                                else:
                                    raise
                        
                        if response:
                            # TR√çCH XU·∫§T gi√° v√†ng
                            gold_price = extract_gold_price(response.text, url)
                            
                            if gold_price:
                                best_source = source
                                logging.info(f"   ‚úÖ [{idx}] Tr√≠ch xu·∫•t th√†nh c√¥ng t·ª´ ngu·ªìn n√†y!")
                                break
                            else:
                                logging.info(f"   ‚ö†Ô∏è [{idx}] Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c, th·ª≠ ngu·ªìn ti·∫øp theo...")
                    
                    except Exception as e:
                        logging.warning(f"   ‚ö†Ô∏è [{idx}] L·ªói fetch: {e}, th·ª≠ ngu·ªìn ti·∫øp theo...")
                        continue
                
                # T·∫°o context d·ª±a tr√™n k·∫øt qu·∫£
                if gold_price:
                    # T·∫°o context NG·∫ÆN G·ªåN t·ª´ gi√° ƒë√£ tr√≠ch xu·∫•t
                    if 'buy' in gold_price and 'sell' in gold_price:
                        tool_context = f"\n\nüí∞ GI√Å V√ÄNG SJC H√îM NAY:\n"
                        tool_context += f"Mua v√†o: {gold_price['buy']} {gold_price['unit']}\n"
                        tool_context += f"B√°n ra: {gold_price['sell']} {gold_price['unit']}\n\n"
                    else:
                        tool_context = f"\n\nüí∞ GI√Å V√ÄNG SJC: {gold_price.get('price', 'N/A')} {gold_price['unit']}\n\n"
                    
                    tool_context += (
                        "‚ö†Ô∏è Y√äU C·∫¶U: Tr·∫£ l·ªùi 1 c√¢u duy nh·∫•t, ƒë·ªãnh d·∫°ng: "
                        "\"V√†ng SJC mua X tri·ªáu/l∆∞·ª£ng, b√°n Y tri·ªáu/l∆∞·ª£ng.\" "
                        "CH·ªà d√πng s·ªë trong ph·∫ßn \"GI√Å V√ÄNG SJC H√îM NAY\" ·ªü tr√™n. "
                        "KH√îNG th√™m l·ªùi khuy√™n, KH√îNG nh·∫Øc ngu·ªìn, KH√îNG d·ª± ƒëo√°n, KH√îNG gi·∫£i th√≠ch.\n\n"
                    )
                    
                    logging.info(f"‚úÖ Gold price extracted and formatted ({len(tool_context)} chars)")
                else:
                    # Fallback: D√πng full content t·ª´ ngu·ªìn t·ªët nh·∫•t v√† t√¨m s·ªë trong ƒë√≥
                    best_source = sources[0]  # D√πng ngu·ªìn ƒë·∫ßu ti√™n
                    url = best_source['url']
                    title = best_source['title']
                    
                    logging.info(f"üì∞ Fetching full article t·ª´ ngu·ªìn t·ªët nh·∫•t: {title[:60]}")
                    
                    try:
                        full_content = fetch_full_article(url)
                        
                        if full_content:
                            # T√¨m t·∫•t c·∫£ s·ªë c√≥ th·ªÉ l√† gi√° v√†ng trong content
                            # Pattern: t√¨m s·ªë trong kho·∫£ng 50-200 tri·ªáu
                            price_numbers = re.findall(r'(\d{1,2}[,\.]\d{1,2})\s*(?:tri·ªáu|tr)', full_content)
                            price_context = ""
                            
                            if price_numbers:
                                # L·∫•y 2-4 s·ªë ƒë·∫ßu ti√™n (c√≥ th·ªÉ l√† mua-b√°n)
                                unique_prices = list(dict.fromkeys(price_numbers[:4]))  # Lo·∫°i b·ªè duplicate nh∆∞ng gi·ªØ th·ª© t·ª±
                                price_context = f"\nüìä C√ÅC S·ªê C√ì TH·ªÇ L√Ä GI√Å V√ÄNG (tri·ªáu/l∆∞·ª£ng): {', '.join(unique_prices)}\n"
                            
                            tool_context = f"\n\nüí∞ TH√îNG TIN GI√Å V√ÄNG H√îM NAY:\n{price_context}\nüìÑ N·ªôi dung b√†i b√°o:\n{full_content[:1500]}\n\n"
                            tool_context += (
                                "‚ö†Ô∏è Y√äU C·∫¶U QUAN TR·ªåNG:\n"
                                "1. T√¨m trong n·ªôi dung b√†i b√°o ·ªü tr√™n, t√¨m gi√° v√†ng SJC mua v√† b√°n (ƒë∆°n v·ªã: tri·ªáu/l∆∞·ª£ng)\n"
                                "2. N·∫øu c√≥ ph·∫ßn \"C√ÅC S·ªê C√ì TH·ªÇ L√Ä GI√Å V√ÄNG\" ·ªü tr√™n, ∆∞u ti√™n d√πng 2 s·ªë ƒë·∫ßu ti√™n (th∆∞·ªùng l√† mua-b√°n)\n"
                                "3. Tr·∫£ l·ªùi 1 c√¢u duy nh·∫•t, ƒë·ªãnh d·∫°ng: \"V√†ng SJC mua X tri·ªáu/l∆∞·ª£ng, b√°n Y tri·ªáu/l∆∞·ª£ng.\"\n"
                                "4. N·∫øu kh√¥ng t√¨m th·∫•y s·ªë c·ª• th·ªÉ, tr·∫£ l·ªùi: \"T√¥i kh√¥ng t√¨m th·∫•y gi√° v√†ng SJC ch√≠nh x√°c trong th√¥ng tin hi·ªán c√≥.\"\n"
                                "5. KH√îNG nh·∫Øc ngu·ªìn, KH√îNG d·ª± ƒëo√°n, KH√îNG gi·∫£i th√≠ch d√†i d√≤ng.\n\n"
                            )
                            logging.warning("‚ö†Ô∏è Could not extract price, using full content with number hints")
                        else:
                            # Fallback cu·ªëi: d√πng snippet t·ª´ t·∫•t c·∫£ ngu·ªìn
                            all_snippets = []
                            for s in sources[:3]:
                                if s.get('snippet'):
                                    all_snippets.append(f"[{s['title'][:50]}] {s['snippet'][:200]}")
                            
                            snippets_text = "\n\n".join(all_snippets)
                            tool_context = f"\n\nüí∞ TH√îNG TIN GI√Å V√ÄNG:\n\n{snippets_text}\n\n"
                            tool_context += (
                                "‚ö†Ô∏è Y√äU C·∫¶U: T√¨m gi√° v√†ng SJC mua v√† b√°n trong th√¥ng tin tr√™n. "
                                "Tr·∫£ l·ªùi 1 c√¢u duy nh·∫•t: \"V√†ng SJC mua X tri·ªáu/l∆∞·ª£ng, b√°n Y tri·ªáu/l∆∞·ª£ng.\" "
                                "N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ l·ªùi: \"T√¥i kh√¥ng t√¨m th·∫•y gi√° v√†ng SJC ch√≠nh x√°c.\"\n\n"
                            )
                            logging.warning("‚ö†Ô∏è Failed to fetch article, using snippets from all sources")
                    
                    except Exception as e:
                        logging.error(f"‚ùå Error fetching full content: {e}")
                        # Fallback: d√πng snippet t·ª´ t·∫•t c·∫£ ngu·ªìn
                        all_snippets = []
                        for s in sources[:3]:
                            if s.get('snippet'):
                                all_snippets.append(f"[{s['title'][:50]}] {s['snippet'][:200]}")
                        
                        snippets_text = "\n\n".join(all_snippets) if all_snippets else sources[0].get('snippet', '')
                        tool_context = f"\n\nüí∞ TH√îNG TIN GI√Å V√ÄNG:\n\n{snippets_text}\n\n"
                        tool_context += (
                            "‚ö†Ô∏è Y√äU C·∫¶U: T√¨m gi√° v√†ng SJC mua v√† b√°n. "
                            "Tr·∫£ l·ªùi 1 c√¢u: \"V√†ng SJC mua X tri·ªáu/l∆∞·ª£ng, b√°n Y tri·ªáu/l∆∞·ª£ng.\" "
                            "N·∫øu kh√¥ng t√¨m th·∫•y, tr·∫£ l·ªùi: \"T√¥i kh√¥ng t√¨m th·∫•y gi√° v√†ng SJC ch√≠nh x√°c.\"\n\n"
                        )
            else:
                tool_context = f"\n\n[L·ªói t√¨m ki·∫øm gi√° v√†ng]"
        
        # ===== CASE 3: C√ÇU H·ªéI KH√ÅC ‚Üí QUY TR√åNH CHATGPT: SEARCH ‚Üí FILTER ‚Üí FETCH ‚Üí ANALYZE ‚Üí SYNTHESIZE =====
        else:
            # B∆Ø·ªöC 1: Web Search - T√¨m ki·∫øm nhi·ªÅu ngu·ªìn
            tool_result = search_web(question, max_results=10)
            
            if 'results' in tool_result:
                final_results = tool_result['results']
                
                logging.info(f"üìä Nh·∫≠n ƒë∆∞·ª£c {len(final_results)} ngu·ªìn t·ª´ search_web")
                
                if final_results:
                    # B∆Ø·ªöC 2: Fetch full content t·ª´ c√°c ngu·ªìn t·ªët nh·∫•t (t·ªëi ƒëa 3 ngu·ªìn)
                    sources_with_content = []
                    
                    for i, source in enumerate(final_results[:3], 1):  # Ch·ªâ fetch top 3
                        title = source['title']
                        url = source['url']
                        snippet = source.get('snippet', '')
                        
                        logging.info(f"   [{i}] {title[:80]}")
                        logging.info(f"       URL: {url[:80]}")
                        
                        # B∆Ø·ªöC 3: Fetch full content (n·∫øu c√≥ th·ªÉ)
                        full_content = fetch_full_article(url)
                        
                        source_data = {
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'full_content': full_content if full_content else snippet
                        }
                        
                        sources_with_content.append(source_data)
                        
                        if full_content:
                            logging.info(f"       ‚úÖ Fetched {len(full_content)} chars")
                        else:
                            logging.info(f"       ‚ö†Ô∏è Using snippet ({len(snippet)} chars)")
                    
                    # B∆Ø·ªöC 4: Ph√¢n t√≠ch v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn
                    tool_context = analyze_and_synthesize(sources_with_content, question)
                    
                    logging.info(f"‚úÖ ƒê√£ t·∫°o context t·ªïng h·ª£p ({len(tool_context)} chars)")
                else:
                    tool_context = ""
                    logging.warning("‚ö†Ô∏è No final results after filtering!")
            elif 'error' in tool_result:
                tool_context = f"\n\n[L·ªói t√¨m ki·∫øm: {tool_result['error']}. H√£y tr·∫£ l·ªùi d·ª±a v√†o ki·∫øn th·ª©c c·ªßa b·∫°n.]"
    
    elif tool_needed == 'calculate':
        # Tr√≠ch xu·∫•t bi·ªÉu th·ª©c to√°n h·ªçc - lo·∫°i b·ªè text, ch·ªâ gi·ªØ ph√©p t√≠nh
        # H·ªó tr·ª£: "3 * 50 b·∫±ng bao nhi√™u", "123*456 =", "t√≠nh 5+8"
        expression = question
        
        # Lo·∫°i b·ªè c√°c t·ª´ kh√≥a th∆∞·ªùng g·∫∑p
        expression = re.sub(r'(b·∫±ng|bao nhi√™u|t√≠nh|l√†|k·∫øt qu·∫£)', '', expression, flags=re.IGNORECASE)
        # Lo·∫°i b·ªè d·∫•u = ·ªü cu·ªëi n·∫øu c√≥
        expression = expression.replace('=', '').strip()
        
        # Tr√≠ch xu·∫•t bi·ªÉu th·ª©c to√°n h·ªçc (s·ªë, to√°n t·ª≠, d·∫•u ngo·∫∑c, kho·∫£ng tr·∫Øng)
        match = re.search(r'[\d\s+\-*/().^]+', expression)
        if match:
            expression = match.group().strip()
            # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            expression = re.sub(r'\s+', '', expression)
            
            # Validate: Ph·∫£i c√≥ √≠t nh·∫•t 1 ph√©p t√≠nh
            if expression and re.search(r'[\+\-\*/\^]', expression):
                tool_result = calculate(expression)
                if 'result' in tool_result:
                    # Format k·∫øt qu·∫£: lo·∫°i b·ªè .0 n·∫øu l√† s·ªë nguy√™n
                    result_value = tool_result['result']
                    if isinstance(result_value, float) and result_value.is_integer():
                        result_display = int(result_value)
                    else:
                        result_display = result_value
                    
                    tool_context = f"\n\nüî¢ K·∫æT QU·∫¢ T√çNH TO√ÅN: {expression} = {result_display}\n\nY√äU C·∫¶U: Ch·ªâ tr·∫£ l·ªùi k·∫øt qu·∫£ s·ªë, KH√îNG th√™m b√¨nh lu·∫≠n hay gi·∫£i th√≠ch. Format: \"[s·ªë]\" ho·∫∑c \"[ph√©p t√≠nh] = [s·ªë]\"."
                    direct_answer = f"{expression} = {result_display}"
                    
                    logging.info(f"‚úÖ Calculator tool context created ({len(tool_context)} chars)")
                    logging.info(f"üìù Result: {expression} = {result_display}")
                else:
                    logging.error(f"‚ùå Calculator failed: {tool_result.get('error', 'Unknown error')}")
            else:
                logging.warning(f"‚ö†Ô∏è Calculator: Kh√¥ng t√¨m th·∫•y ph√©p t√≠nh h·ª£p l·ªá trong c√¢u h·ªèi")
    
    elif tool_needed == 'code':
        # Tr√≠ch xu·∫•t code t·ª´ c√¢u h·ªèi (n·∫øu c√≥ code block)
        code_match = re.search(r'```python\n(.*?)\n```', question, re.DOTALL)
        if code_match:
            code = code_match.group(1)
            tool_result = execute_code(code)
            if 'output' in tool_result:
                tool_context = f"\n\nüíª K·∫æT QU·∫¢ CH·∫†Y CODE:\n{tool_result['output']}\n\nY√äU C·∫¶U: Gi·∫£i th√≠ch k·∫øt qu·∫£ code tr√™n b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch ng·∫Øn g·ªçn."
                
                logging.info(f"‚úÖ Code execution tool context created ({len(tool_context)} chars)")
            else:
                logging.error(f"‚ùå Code execution failed: {tool_result.get('error', 'Unknown error')}")
    
    elif tool_needed == 'clarify':
        # Multi-turn clarification - H·ªèi l·∫°i khi c√¢u h·ªèi m∆° h·ªì
        logging.info("üí¨ Clarification needed")
        tool_context = "\n\nüí¨ C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ch∆∞a r√µ r√†ng. H√£y h·ªèi l·∫°i ƒë·ªÉ l√†m r√µ:\n"
        tool_context += "VD: 'B·∫°n mu·ªën bi·∫øt th√¥ng tin g√¨ c·ª• th·ªÉ?', 'B·∫°n ƒëang h·ªèi v·ªÅ c√°i g√¨?', 'B·∫°n c√≥ th·ªÉ n√≥i r√µ h∆°n ƒë∆∞·ª£c kh√¥ng?'"
    
    # N·∫øu c√≥ c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp (calculator) ‚Üí tr·∫£ v·ªÅ lu√¥n, kh√¥ng g·ªçi LLM
    if direct_answer is not None:
        answer = direct_answer
        add_to_memory(session_id, 'user', question)
        add_to_memory(session_id, 'assistant', answer)

        if not use_stream:
            return jsonify({'reply': answer, 'response': answer})

        def calc_stream():
            for char in answer:
                yield f"data: {json.dumps({'content': char})}\n\n"
            yield f"data: {json.dumps({'done': True})}\n\n"

        logging.info("‚úÖ Returning direct calculator answer without calling LLM")
        return Response(
            stream_with_context(calc_stream()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
        )

    # L·∫•y l·ªãch s·ª≠ (n·∫øu ch∆∞a l·∫•y)
    if 'history' not in locals():
        history = get_conversation_history(session_id)
    
    # T·∫°o messages (l·ªçc b·ªè timestamp)
    msgs = [{'role': 'system', 'content': SYSTEM_PROMPT}]
    for msg in history:
        msgs.append({
            'role': msg['role'],
            'content': msg['content']
        })
    
    # Th√™m tool context v√†o c√¢u h·ªèi n·∫øu c√≥
    final_question = question + tool_context if tool_context else question
    
    if tool_context:
        logging.info(f"üîó Final question length: {len(final_question)} chars (original: {len(question)})")
    
    msgs.append({'role': 'user', 'content': final_question})
    
    # N·∫øu KH√îNG streaming ‚Üí tr·∫£ v·ªÅ JSON b√¨nh th∆∞·ªùng
    if not use_stream:
        lm_resp = query_lm_studio(msgs, stream=False)
        
        if not lm_resp:
            return jsonify({'error': 'L·ªói k·∫øt n·ªëi LM Studio'}), 500
        
        try:
            answer = lm_resp['choices'][0]['message']['content']
            answer = clean_response(answer)
            
            # L∆∞u memory
            add_to_memory(session_id, 'user', question)
            add_to_memory(session_id, 'assistant', answer)
            
            return jsonify({
                'reply': answer,
                'response': answer
            })
        except Exception as e:
            logging.error(f"L·ªói x·ª≠ l√Ω response: {e}")
            return jsonify({'error': 'L·ªói x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi'}), 500
    
    # STREAMING MODE - Tr·∫£ v·ªÅ t·ª´ng chunk
    def generate():
        lm_resp = query_lm_studio(msgs, stream=True)
               
        if not lm_resp:
            yield f"data: {json.dumps({'error': 'L·ªói k·∫øt n·ªëi LM Studio'})}\n\n"
            return
        
        full_answer = ""
        
        try:
            # ƒê·ªçc t·ª´ng d√≤ng t·ª´ streaming response
            for line in lm_resp.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    
                    # LM Studio tr·∫£ v·ªÅ format: "data: {...}"
                    if line_text.startswith('data: '):
                        json_str = line_text[6:]  # B·ªè "data: "
                        
                        if json_str.strip() == '[DONE]':
                            break
                        
                        try:
                            chunk_data = json.loads(json_str)
                            
                            # L·∫•y content t·ª´ chunk
                            if 'choices' in chunk_data and len(chunk_data['choices']) > 0:
                                delta = chunk_data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                
                                if content:
                                    full_answer += content
                                    
                                    # G·ª≠i chunk ƒë·∫øn frontend
                                    yield f"data: {json.dumps({'content': content})}\n\n"
                        
                        except json.JSONDecodeError:
                            continue
            
            # K·∫øt th√∫c stream
            yield f"data: {json.dumps({'done': True})}\n\n"
            
            # L∆∞u v√†o memory
            full_answer = clean_response(full_answer)
            add_to_memory(session_id, 'user', question)
            add_to_memory(session_id, 'assistant', full_answer)
            
        except Exception as e:
            logging.error(f"L·ªói streaming: {e}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no'
        }
    )





# ===== MAIN =====

if __name__ == '__main__':
    print("\n" + "="*70)
    print("ü§ñ AI 37 CHATBOT - PHI√äN B·∫¢N N√ÇNG CAO")
    print("="*70)
    print(f"üì° LM Studio: {LM_STUDIO_URL}")
    print(f"üß† Model: {LM_STUDIO_MODEL}")
    print(f"üöÄ GPU: ƒê√£ b·∫≠t (gpu_layers=35)")
    print(f"üåê Server: http://localhost:{SERVER_PORT}")
    print("="*70)
    print("‚ú® GIAI ƒêO·∫†N 1:")
    print("   ‚ö° Streaming Response - G√µ t·ª´ng ch·ªØ nh∆∞ ChatGPT")
    print("   üíæ L∆∞u l·ªãch s·ª≠ - Kh√¥ng m·∫•t khi restart")
    print("   üîÑ T√≥m t·∫Øt t·ª± ƒë·ªông - Nh·ªõ h·ªôi tho·∫°i d√†i")
    print("="*70)
    print("üî• GIAI ƒêO·∫†N 2:")
    print("   üîç Web Search - T√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t (DuckDuckGo)")
    print("   üî¢ Calculator - T√≠nh to√°n to√°n h·ªçc (Sympy)")
    print("   üíª Code Execution - Ch·∫°y Python code an to√†n (Sandbox)")
    print("   üí¨ Multi-turn Clarification - H·ªèi l·∫°i khi c√¢u h·ªèi m∆° h·ªì")
    print("="*70)
    print("‚ö†Ô∏è  ƒê·∫£m b·∫£o LM Studio ƒëang ch·∫°y v·ªõi model: vistral-7b-chat@q8")
    print("‚ö†Ô∏è  B·∫≠t GPU trong LM Studio Settings ƒë·ªÉ tƒÉng t·ªëc")
    print("="*70 + "\n")
    
    # Ch·∫°y Flask - t·∫Øt auto-reload ƒë·ªÉ tr√°nh l·ªói .env
    app.run(
        host=SERVER_HOST,
        port=SERVER_PORT,
        debug=False,
        threaded=True,
        use_reloader=False
    )
